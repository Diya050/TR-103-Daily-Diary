# DAY - 7

## Job roles

The AI job market has been growing at a phenomenal rate for some time. Here are some job roles to consider if you want to work with artificial intelligence technology. In each description, you’ll find information about what people do and the skills they use to do it.

- **Machine learning engineer:** <br>
Machine learning engineers are at the **intersection of software engineering and data science**. They use big data tools and programming frameworks to create production-ready and scalable data science models that can handle terabytes of real-time data.

Machine learning engineer jobs are best for anyone with a background that combines data science, applied research, and software engineering. You can thrive if you have strong mathematical skills, experience in machine learning, deep learning, neural networks, and cloud applications, and programming skills in Java, Python, and Scala. It also helps to be well-versed in an integrated development environment (IDE), like IBM Watson Studio.

- **Data scientist:** <br>
Data scientists use machine learning and predictive analytics to gain insights from large amounts of data. To prepare, you should build your expertise in **big data platforms and tools**, perhaps including Hadoop, Pig, Hive, Spark, and MapReduce. It would be helpful if you are fluent in at least two programming languages, including structured query language (SQL), Python, Scala, and Perl. You should also invest some time learning descriptive and inferential statistics.

This is a field in which most people have earned a master's or doctoral degree. You would also benefit from non-technical workplace skills, like communication, collaboration, intellectual curiosity, and business acumen.

- **Business intelligence developer:** <br>
Business intelligence (BI) developers design, model, build, and maintain data sets for complex data platforms. Their primary job is to analyze complex data and look for current business and market trends to help increase the profitability and efficiency of their organization. Strong technical and analytical skills would help you break into this field, as well as collaboration and problem-solving skills.

For this career, you should consider earning a bachelor’s degree in computer science, engineering, or a related field, or a combination of certifications and on-the-job experience. Companies usually prefer candidates with **experience in data warehouse design or with data mining**, and knowledge of BI technologies, **SQL queries, SQL Server Reporting Services (SSRS), and SQL Server Integration Services (SSIS)**.

- **Robotic scientist:** <br>
Robots can automate jobs, but they require programmers working behind the scenes to ensure they function well. Robotics engineers build and maintain AI-powered robots. Their job is to design and build mechanical devices that can perform tasks with commands from humans. To succeed in robotics, you should learn to code in various programming languages and to develop working prototypes.

To prepare for a career in robotics, you might earn a bachelor’s degree in one of the following disciplines: robotic engineering, mechanical engineering, electro-mechanical engineering, or electrical engineering. Companies also look for professionals with specializations in advanced mathematics, physical sciences, life sciences, computer science, computer-aided design and drafting (CADD), physics, fluid dynamics and materials science, and a related AI certification.

- **Software engineer:** <br>
AI software engineers **build software products for AI applications**. AI software engineers develop and maintain the software that data scientists and architects use. They stay informed and updated about new artificial intelligence technologies.

As an AI software engineer, you’d be skilled in software engineering and artificial intelligence. You’d have programming skills, statistical skills, and analytical skills. Companies typically look for a bachelor’s degree in computer science, engineering, physics, mathematics, or statistics.

- **Natural language processing engineer:** <br>
Natural language processing (NLP) engineers are AI professionals who specialize in human language, including spoken and written information. The engineers who work on voice assistants, speech recognition, document processing, and so on use NLP technology.

For the role of an NLP engineer, organizations expect you to have a specialized degree in computational linguistics or a combination of computer science, mathematics, and statistics.

## Skills to build

- **Baseline Skills:**
     - Linear algebra
     - Probability
     - Statistics
     - Signal processing
     - Big data
- **Workplace Skills:**
     - Communication skills
     - Teamwork and collaboration
     - Problem solving
     - Decision making
     - Analytical thinking
     - Time management
     - Business intelligence
     - Critical thinking

- **Advanced technical skills:**
     - Programming languages (Python, R, Java, C++)
     - Frameworks and libraries (TensorFlow, SciPy, Numpy)
     - Neural networks
     - Machine learning
     - Deep learning
     - Shell scripting
     - Cluster analysis
     - Tableau
     - Microsoft Power BI

## Machine learning models

Machine learning is, in the end, about making predictions using **statistics and calculus**; both of which are used in bits of code called **machine learning algorithms**. These bits of code, in turn, can be organized in large-scale computer programs called **machine learning models**. Your understanding of machine learning and how it’s executed in **IBM Watson Studio** starts with an exploration of these two concepts.

- **Machine Learning Algorithms:** A machine learning algorithm is a set of program code. If you’ve ever worked with code, you might know what a function is. A function is a set of logical operations that inputs some sort of data, analyzes or transforms it, and then outputs a result. But in a machine learning algorithm, that **analysis often has a specific goal: to recognize patterns in data sets**. For example, an algorithm in an AI weather prediction system might ingest a series of sunlight measurements and water temperatures, then analyze and output a pattern describing how these factors appear to influence each other.
- **Machine Learning Models:** A machine learning model is a group of machine learning algorithms. Operating together, they detect patterns among their algorithms’ output and use those patterns **to make predictions**. For example, a model whose algorithms look at patterns regarding temperature, climate, geography, and so on might predict a rainy-day next Saturday.

How is this different from the way conventional computer programs operate? A machine learning model doesn’t depend only on a human to write its code or to adjust its programming if its predictions aren’t right. Instead, **a machine learning model can reprogram itself**. So if, for example, a weather model tends to get a certain type of prediction wrong, it can adjust its algorithms (weights and biases, statistical constructs) to improve the accuracy of its predictions.

Why are you learning about this? Because creating machine learning models is the primary job of IBM Watson Studio. Its integrated development environment (IDE) makes this difficult task easier, faster, and more affordable. This is the future of computer programming.

## Watson Studio was born from a need

Until about 2015, artificial intelligence scientists found that building AI systems was **cumbersome and time-consuming**. They might need to search the web, review components, hand-code connections to big data tools and learn to use them, and more. This was an **expensive problem** for laboratories and companies trying to develop and market useful systems.

Even worse, working with AI **broke up workflows**. Developers toggled from one technology to another, jumping back and forth between workspaces, and repeatedly switching programming tools to get tasks done.

These problems slowed down the adoption of AI systems, making it **hard to integrate them into existing technology** such as medical care or automobiles. Once all the elements were working together and launched, it was difficult to provide support when help was needed.


## IBM Watson Studio fulfills the need

IBM’s Watson Studio solution is what researchers call an integrated development environment (IDE). Named after IBM’s founder, it pulls together the most useful development and analytic tools, wrapping them in a development platform that is powerful enough to meet large-scale challenges, yet simple enough that developers can master it quickly.

IBM Watson Studio solves AI development problems. With IBM Watson Studio, businesses can simplify data projects with a streamlined process to extract insights from data to help them get smarter faster. It delivers an easy-to-use, collaborative data science and machine learning environment to build and train models, prepare and analyze data, and share insights all in one place.

IBM Watson Studio is a single environment for sharing work to solve problems within the system, rather than starting from scratch every time a new issue arises. And developers can use that efficiency to quickly dive into building machine learning and deep learning algorithms. 

Watson Studio gives developers:

- A collaborative data science and machine learning environment
- Easy visualizations with drag-and-drop code
- An efficient workflow
- A built-in neural network modeler
- Open-source tools such as Jupyter Notebooks and RStudio

Watson Studio’s features sound impressive. But do they really make a difference? Read on to learn how they help developers working on real-world projects.

## IBM Watson Studio speeds data input and insights output

Watson Studio is more than a collection of resources that can communicate with each other. It eases ingesting data into an AI system, organizes assets, performs complex analysis, and displays results that make complicated issues simpler to understand.


- **Automated data preparation:** 
IBM Watson Studio has a feature called AutoAI that prepares raw data for machine learning. It can apply various algorithms to clean and structure the data, automatically select an appropriate model, and optimize its output for the fastest, most useful output.

- **Visual neural network design:** 
Watson Studio helps developers create machine learning flows and design neural networks visually, using a simple drag-and-drop interface and open source code libraries.

- **Sophisticated analysis and prediction:** 
Watson Studio recommends algorithms and uses the latest neural networks to predict and build patterns. It helps programmers create visualizations simply by selecting data items, after which the system itself can choose the best way to visualize its findings.


- **Unified dashboard displays:** 
Watson Studio dashboards not only visualize the results of complex analyses, they also gather related views on data into a single place where clients can find and understand the information they seek. These dashboards don’t require specialized coding or database skills, and researchers can easily share them across the internet.

IBM Watson Studio is designed to make data science a team sport that helps developers learn, create, and collaborate on great machine learning models. 

## When a classical computer program is not enough

Today businesses, industries, and governments need more computing power than seemed enough just a few years ago. They depend on data analysis that can only be developed from machine learning models, using tools like IBM Watson Studio.

Consider, for example, the problem faced by a sneaker manufacturer that wants to know what customers think and say about its products. This information could help it plan product distribution for today and keep up with new trends in the future. The best way to discover these things might be to study product reviews people have posted, and also study IM texts sent by the manufacturer’s teen and young adult customers. What are they saying? What do they love or hate? What new thing did they just see that they really want to buy?

Here are some of the things that make this discovery difficult.

- Millions of social media posts hit the internet every hour across Twitter, TikTok, and others.
- Those posts cover zillions of topics, most of which have nothing to do with sneakers.
- Posts that do mention sneakers might do so in countless different contexts.

Even if a conventional computer could be programmed and given enormous resources to keep up with this flood of data, sneaker fashions change without warning—and those changes cannot be predicted or prepared for with traditional program code.

But with IBM Watson Studio, a modest-size team of specialists could build a machine learning model that will, as time goes by, get better and better at spotting major trends in sneaker design. It could even follow celebrity fashions, then begin predicting new trends more and more effectively.

# DAY - 8

## Your AI machine learning project

Imagine that you’re an up-and-coming coder looking for your big break. Yesterday, you got a phone call from a major bank in Germany. A few hours later you found yourself flying across the ocean on a transcontinental jet! This could be your chance to make your name in AI machine learning. You leaned your seat forward and studied the materials that the bank sent you.

## A terrific opportunity

The bank wants to protect itself from the risk of making loans that are never repaid.

- It’s preparing to launch a new website on which customers can submit requests for loans up to €10,000 and, if their credit is sound, get instant approval.
- The good news is that this will **attract customers** who want to apply for loans quickly, **without a lot of fuss**.
- The bad news is that it will give the **bank almost no time to investigate **each applicant’s credit risk, then reject those who might default on their loan, ultimately costing the bank millions of euros.

The bank wants you to lead the development of an AI machine learning system that can **rapidly predict the risk of granting a loan** for each customer who applies.

## The scope of your experiment

Using simulations of IBM Watson Studio, you’ll build an AI machine learning model, train algorithms on the same data set of bank loans and defaults, then test them competitively, using additional data, to see which predicts defaults most accurately.

## Your data set

You’ll use data from a real German bank that’s fictionalized for this simulation.
The file contains records of five thousand past loans.
Each record includes ample data such as the customer’s credit rating, bank account balances, loan purpose, and more—and whether the borrower paid or defaulted on the loan.

## Your to-do list
Using IBM Watson Studio and a data set, you will simulate the following steps.

- Set up a new project in IBM Watson Studio.
- Create a learning model designed to predict whether an applicant is more likely to pay back or default on a loan.
- Train your model on 90% of your data set, including loan outcomes.
- Test the model on a test set of data (the remaining 10% of the data in the data set) to see which algorithm gives the best predictions.
- Save the final model with highest confidence as a Jupyter notebook.


### Start a project and upload data 
1. After you log into IBM Cloud, you’ll find the Dashboard. This Dashboard lets you access the 
tools, services, resources and more in your IBM Cloud account. Select Catalog on the top 
toolbar. 
The IBM Cloud dashboard 
With several bank IT workers looking on, you log into IBM Cloud. As it comes up, you 
explain that you can access Watson Studio in just a few steps through an IBM Cloud 
account. This acts as your doorway to amazing AI resources. 
2. This is the catalog of IBM products and services. It is a best practice to create database storage 
first. Type “object storage” in the Search field and select Enter 
3. Select Object Storage. 
4. For this simulation, you are using a free account, so you don’t need to review a pricing plan. 
Select the Next arrow to continue. 
5. Scroll down to the Configure your resource section. Developers would say that you’re going to 
“provision” the object storage. A Service name is automatically generated, but you should name 
it something meaningful to your project. Type “Cloud Object Storage-Risk_Fraud” in the Service 
name field and select Enter. 
6. You don’t need to fill out the other fields. In the lower right select Create.  
7. Now that you’ve provisioned the Cloud Object storage, select the sandwich icon at the top left 
to display the navigation menu. 
8. Select Resource list from the navigation menu. 
9. This displays a list of the resources that you have created. Select Storage to see that the Cloud 
Object Storage_Risk_Fraud that you just created is listed. 
10. Select Catalog on the top toolbar. 
11. Notice on the left side under Category that there is a large collection of available tools. Select 
the AI / Machine Learning category. 
An AI / machine learning service 
Now you’re through the IBM doorway. You’ve finished basic bookkeeping and you’re 
ready set up a project in IBM Watson Studio. Some of the IT folks haven’t seen anything 
like this before, so more of them crowd around to watch you get ready. 
12. Notice there are several AI and machine learning products listed. Find and select the Watson 
Studio block.  
13. For this simulation, you are using a free account, so you don’t need to review a pricing plan. Note 
that the location selected is Dallas (us-south). Scroll down to reveal the Configure your 
resource section, then select the Next arrow to continue. 
14. Under the Configure your resource section, the Service name is automatically generated, in 
this case Watson Studio-dl. Type “Watson Studio-AI Fundamentals” in the Service name field 
and select Enter. 
15. You don’t need to fill out the other fields. In the lower right, assume that you’ve checked that 
you have read and agree to the license agreements, then select Create. 
16. You’ve provisioned the IBM Watson Studio service! Notice the service name you picked displays 
in the upper left. Now, you need to create a new project. Select Launch in IBM Cloud Pak for 
Data.  
17. On the Build and manage ML models pop-up, Provision Watson Machine Learning is select by 
default. Select Next. 
A new AI project 
With your service set up, you’ll create the AI project itself. This involves launching it, 
naming it, and associating it with the database that you’ll use to test your machine 
learning models. 
18. Make sure that Dallas displays in the Select a region field. Since Dallas was selected for 
Watson Studio, you must make sure that you select Dallas for all the other services. Select the 
Next arrow to continue. 
19. Under the Configure your resource section, name your service. Type “Machine Learning
Risk_Fraud” in the Service name field. Then select Create. 
20. You are working on a new project, so select New project to create an empty project and then 
select Next. 
21. Under the Define details section, you can name your project. Type “Risk_Fraud” in the Name 
field. Select the Next arrow to continue. 
22. Notice under Storage that IBM Watson Studio already provisioned the Cloud Object Storage 
(COS) that you created earlier so you have a database. This COS can store unstructured data like 
images and text. Soon, you’ll work with structured data in a comma-separated values (CSV) file. 
Select Create.  
23. You’ve created the project! This is the IBM Watson Studio Projects dashboard. You can see your 
project name at the top. This is the Overview tab. Select the Assets tab.  
24. It’s time to upload your data set. Select Drop data files here to open your local drive and find 
the german_credit_data_biased_training.csv file that you’ll be working with for the project. 
25. Drag and drop the german_credit_data_biased_training.csv file to Drop data files or browse 
for files to upload under Data in this project. 
26. Notice your CSV file is right there for you. Select “german_credit_data_biased_training.csv”. 
27. A Preview assets page opens so you can now preview your data. Take a moment to check out 
the data that displays. Select the Next arrow to continue.


### Create and run AI models 
1. Now that you’ve set up your data set in cloud storage, it is time to build the AI models. Select New 
asset to start creating your AI model. 
AI models 
Creating artificial intelligence models used to be an incredibly complex and difficult task. 
But today you can show off to your colleagues how IBM Watson Studio does this for you—
 automatically! 
2. On the left side, there is a Tool type list. Select the Automated builders tool. AutoAI is the only 
Automated builder displayed. With AutoAI, you will be able to quickly set up and run AI models 
using your data to train and test the models. Select AutoAI. 
3. Creating and testing AI models is called experimenting. Now it’s time to create your AutoAI 
experiment. Type “Loan Risk” in the Name field to name your model and press Enter. 
4. In the Define configuration section, you’ll see a machine learning service isn’t associated with your 
project yet. Let’s take care of that now. Select the Associate a Machine Learning service instance 
link. 
5. On the Associate service page, you’ll see all the services you’ve created that can be associated with 
your experiment. Right now, there’s only one service displayed: Machine Learning-Risk_Fraud. 
Select the checkbox next to the service and then select Associate. 
6. Now that you’ve associated your machine learning service, select Reload to refresh the page. 
7. You can see Machine Learning-Risk_Fraud displayed in the Watson Machine Learning Service 
Instance field now. You’re ready to create your AutoAI experiment. Select Create. 
8. It’s time to add your data source. Select Select from project to find the data source you added 
previously.  
Your experiment 
You promised the bank you’d run several different machine learning algorithms competitively. 
You’d train them all on the same partial set of historic data about people who took out loans 
and then paid them back or defaulted on them. Then you’d test how well they performed, based 
on what they’d learned, by feeding them identical sets of new data, without telling them who 
defaulted. Each algorithm would make predictions about everyone in the new data set, 
classifying people as Risk or No Risk. 
9. On the left, you’ll see Categories listed. Select Data asset to reveal your data assets.  
10. You can see your data asset. Select the check box next to: german_credit_data_training.csv.  
11. On the right, information about the asset is displayed including: name, asset type, size, and when it 
was modified and created. Select Select asset. 
12. Now it’s time to configure the details for your experiment. The first question that displays asks if you 
want to create a time series forecast. For this experiment, you don’t want to do that, so select No. 
13. The next question that displays asks what you want to predict. This is asking what column from your 
data set you want the AI model to predict. Select Select prediction columns to open the drop-down 
list. 
14. Next to Risk are the letters “STR”. This indicates that the data type in the Risk column is a String. 
Strings are a sequence of letters, digits, punctuation, and so on. In this case, it’s text data. Since 
you’re trying to predict which individuals are good risks for loans, select Risk.  
15. The PREDICTION TYPE that Watson AutoAI selected is Binary Classification. You’re trying to 
determine whether an individual presents a Risk or No Risk, which is a classification with only two 
options, so leave the prediction type as Binary Classification. Select the Next arrow to continue. 
16. By default, Accuracy & run time is selected in the OPTIMIZED FOR field. This means when Watson 
is evaluating which algorithm is best it will optimize by balancing accuracy and speed. Select 
Experiment settings to make further changes.  
17. This is the General tab. It shows what you selected on the previous page. Binary classification is 
the prediction type, No Risk is the positive class, and Accuracy is the optimize metric. Select the 
Next arrow to continue. 
Competitive algorithms 
Some folks watching you had expected you to create mathematical algorithms that could 
analyze data and predict credit risk. It’s time to astonish them. How? By using Watson 
Studios to line them up quickly. 
18. Scroll down to view the Algorithms to include section, notice Gradient Boosting Classifier is not 
selected. Select the check box next to Gradient Boosting Classifier. 
19. Scroll down to view the Algorithms to use section. Watson AutoAI lets you test up to four 
algorithms. Select the Next arrow to continue. 
20. By default, Watson will select the best two algorithms to test on the data. Select Data source from 
the left menu. 
21. Scroll down to view the Training and holdout method section.  
The Training data split slider lets you choose how much of the data set to use for training 
and how much to use for testing. By default, 90% of the data set is used for training and 
10% of the data set is used to test how the AI model is performing. Hover over the 
underlined words to see more about each field in this section. 
22. Below the Training data split is the Select features to include section. Here, you can select which 
columns (or features) in the data set to use for training and testing. There are three pages of data. 
Select the forward arrow to page through the columns in the data set. 
23. All the features in the data set are available except Risk because that’s what is being tested. By 
default, all the features are selected. Select the check box next to Telephone to deselect it and not 
use this feature in the experiment. 
24. Select Save settings to confirm all your choices. 
25. Everything is set up! Select Run experiment to start the experiment. 
Launch time! 
You’re about to launch the experiment. Your models will ingest the historic training data, 
then run the new data and make their predictions. Like people watching a horse race, 
everyone leans in to see what happens. 
26. The Relationship map shows by default. Select the Swap view link under Progress map to display 
the Progress map.  
27. Here, the Progress map shows the two models that are being tested. Take a moment to review. 
Select the Next arrow to continue. 
28. Watch as the AutoAI experiment runs. This will only take 45 seconds, but would take roughly 5 
minutes when performed in the live environment. 
29. Notice that each model has four pipelines (or algorithms) for a total of eight pipelines. The pipeline 
that’s producing the most accurate predictions and is the fastest is shown with a star.  
Select the pipeline with the star. 
30. This displays the Pipeline details. By default, the ROC curve is shown. Select Confusion matrix 
from the Model viewer on the left. 
31. This displays the Confusion matrix. You will learn more about the confusion matrix and what it 
means next.

Conclusion: You successfully ran an AutoAI experiment in IBM Watson Studio. You determined which of 
the eight pipelines makes the most accurate predictions based on the data set and generated a 
confusion matrix.  
Conclusion: You successfully provisioned the IBM Watson Studio service, set up a new project, and 
uploaded the data set to your project so you’re ready to begin working with it. 


## Save AI models as Jupyter Notebooks 
1. Once you have completed your experiment and determined the best model, your next step is to save 
the model. Select Save as. 
Models and notebooks 
You’ll recall that the experiment ended by showing a confusion matrix. It’s a mathematical 
test that compares predictions from all of the algorithms to see which one worked best. 
Basically, your job is done now. The IT folks who had gathered around begin to disperse—
 except for a couple of people who linger so they can see how you’ll save and store the 
winning model. 
Select X to close this window and continue. ⓘ 
2. You have two options. You can save the model or you can save as a notebook. To use the model 
again in Watson Studio you would select Model, but to interact with the model on a deeper level you 
need to save it as a notebook. Select Notebook. 
3. The name shown in the Name field in the Define details section will change to the name of the model 
and the pipeline that Watson selected with “Notebook” at the end. Select Create. 
4. Now, select the View in project link that displays in the notification box. 
5. The Pipeline notebook displays, but it is currently view only. You need to be able to edit it. Select 
Edit icon from the menu on the top right. This makes it possible to edit the notebook. 
6. The notebook in an editable form displays, but it is not trusted. This means that the JavaScript shown 
in the notebook will be disabled and not allowed to run. Since you want to be able to run the code, 
you need to trust the notebook. Select the Not Trusted link in the upper right. 
7. This displays the Trust this notebook? dialog box. Select Trust. 
8. The display refreshed and the notebook now shows as trusted and all the JavaScript in the notebook 
is enabled. Select the Next arrow to continue. 
9. Now that your notebook is trusted, it’s time to download the notebook. Select File from the top menu. 
10. Now, select Download as. This displays the formats that you can download the notebook in. 
11. Select Notebook (.ipynb) to download the notebook in Jupyter Notebook format. 
12. This will open a new tab and automatically download the notebook in Jupyter Notebook format to your 
computer. Next, you’ll look at what you can do with the downloaded notebook.

Conclusion: You successfully saved and downloaded your IBM Watson AutoAI model. You can now use 
this model with new data sets, modify the code of the model, and even use the model on other integrated 
development environments (IDEs). 

# DAY - 9

## What is prompt tuning?

**Large language models** like ChatGPT **are flexible and can perform various tasks** such as analyzing legal documents or writing poems. Previously, **fine tuning was used to improve the performance of pre-trained models by gathering and labeling examples of the target task**. However, a more energy-efficient technique called prompt tuning has emerged as a simpler alternative. **Prompt tuning allows tailoring a massive model to a narrow task without requiring a large number of labeled examples.** 

It involves **feeding task-specific cues or prompts** to the model, which can be human-generated or AI-generated. P**rompt engineering is the development of prompts that guide models to perform specialized tasks.** Soft prompts, generated by AI, have been shown to outperform human-engineered prompts and are used in prompt tuning. They act as a substitute for additional training data. **Prompt tuning and soft prompts lack interpretability but are effective in guiding the model towards desired outputs.** Prompt tuning is proving to be a game changer in multitask learning and continual learning, allowing for swift adaptation to specialized tasks and concepts.

## Can AI help climate change?

This video discusses the intersection of artificial intelligence (AI) and climate change, specifically focusing on the role of AI in developing new materials to address the issue. The goal is **to mitigate climate change by developing low carbon technologies**.

Existing technologies face **challenges related to performance, toxicity, and stability, which impact their adoption and cost.** The **complexity of exploring various materials, processes, and operating conditions in a traditional laboratory** setting makes the problem difficult to solve. AI can play a crucial role by speeding up and enhancing the scientific method. It can **aid in generating hypotheses through natural language processing, analyzing vast amounts of scientific literature, and making meaningful connections between materials and properties. AI techniques, such as generative modeling, machine learning, and quantum chemistry, can predict and assess new materials and their performance before physical experimentation.**

AI-enhanced **simulations allow for efficient testing of materials under different conditions**, leading to the discovery of promising candidates. AI can also **optimize synthesis routes and automate laboratory processes, generating data to improve models and expand the scientific knowledge graph.** The integration of AI with the scientific method is expected to accelerate material discovery and have a significant impact on addressing climate change in the coming years.


# DAY - 10

## Natural Language Processing and Computer Vision

Every day, **computers** help you by understanding things they **hear and see.** Your phone responds to your verbal questions and commands. Traffic management systems observe streets and highways to suggest ideal routes for you to travel. Computers respond to your needs and help you make good decisions! Think of artificial intelligence (AI) as prediction machines that augment human intelligence and provide insights.

### The Debater Project

If you ask an artificial intelligence system a question on a topic for which it’s been trained, it can look up an answer. The quiz show, Jeopardy!, proved that years ago. But can an AI system carry on a continuing conversation and respond to what is said with each exchange? Let’s make this question even more difficult: **Can an AI win a debate with a human expert about a complicated topic?** To answer this question, consider a famous experiment, then learn how AI works with spoken and written language.

IBM began developing **Project Debater** in 2012, hoping to build a machine that could do more than win debates with humans. In other words, IBM Project Debater would need to be able to do more than just answer questions in a human language; it would need to have the ability to listen to a series of competitive arguments posed by humans and respond to them intelligently. **IBM's ultimate goal was to build a system that could help people make evidence-based, bias-free decisions on difficult topics where the answers aren’t obvious.**

#### It takes four steps to win a debate
**Step 1. Learn and understand the topic**
Ingest several billion passages from newspapers, books, and journals. (AI researchers call this collection of learned material a corpus.) Then, structure all that content so you can relate concepts to each other and evaluate them, even when they’re stated in different ways.


**Step 2. Build a position**
Create an opening speech made of short pieces of text pasted together from the corpus. Your speech should detail your position on the debate topic. It can’t be a jumble of phrases. It must present a compelling argument, in logical order, using good grammar.


**Step 3. Organize your proof**
Learn the deeper meaning of the facts that surround your topic. Decide what evidence is strongest and arrange your proof by themes. Adjust your arrangement each time new evidence arrives. This will help you find updated or completely new information that can score points against your opponent’s position.


**Step 4. Respond to your opponent**
Listen to your opponent’s arguments and rebuttals, then deliver a convincing rebuttal that refutes your opponent and further proves your case.

Why do we care about these four steps? Because they reflect and test four things that cognitive systems do:

![image](https://github.com/user-attachments/assets/153ce583-8392-4469-9f04-7e5c203342dc)


### Is it difficult to understand human language?

Understanding human language is difficult, even for people who have grown up with it. Human language is incredibly complex, full of strange expressions that seem to contradict each other, metaphors that require cultural knowledge to understand, and grammatical structures that sometimes turn simple ideas into tongue-twisters. Machines require systems that research scientists call natural language processing, or NLP, to understand human language. IBM Project Debater was the most complex AI system IBM Research had ever built to understand human language.

## NLP sentence segmentation and tokens

Computers are best at working with structured data, in which everything is neatly grouped and labeled. Unfortunately for machines, human language is anything but structured. You’ve been using language for most of your life. Your brain accomplishes this through some of the most complicated neural circuitry on Earth. But it is very difficult to create machines that can work with human language.

### NLP, machines segment sentences and extract meaning from “tokens” of human language

Human language is **unstructured**. Although it is **loosely held together by rules of grammar**, our language expresses information in many confusing ways. Unlike structured information, which can be arranged in tables or matrices with neatly labeled rows and columns, unstructured information is **messy and difficult to understand**. To see why, consider this famous joke by Groucho Marx.

> One morning I shot an elephant in my pajamas. How he got in my pajamas, I don’t know.
> Adapted from Groucho Marx, 20th century comedian and movie star

To deal with the **“messiness” of unstructured information**, computers begin with **one sentence at a time. This is called sentence segmentation**. Computers then break the information into **small chunks of information, called tokens**, that can be individually classified. Once the tokens in text have been sorted into a structure based on what they mean, NLP can work with them.

The following activities show you how Groucho Marx’s joke can be tokenized into useful categories called **entities and relationships**. You’ll learn the meanings of these words as you continue.

An **entity** is a noun representing a person, place, or thing. It’s not an adjective, verb, or other article of speech. I, elephant, and pajamas are entities because they are nouns. Shot, an, in, and my are not entities because they are not nouns.

A **relationship** is a group of two or more entities that have a strong connection to one another.
For I + elephant, I + pajamas, and elephant + pajamas, both words in the pair are nouns and are related entities.
I + shot is not a relationship between entities. I is a noun but shot is a not.
in + pajamas is not a relationship between entities. Pajamas is a noun but in is not.

Once an AI has classified entities and relationships in text or speech, the AI can begin structuring the information as a step toward understanding it. Your brain, by the way, does the same thing, which might have helped you find entities and relationships in the previous activities.

For example, consider the following two sentences: “Armen broke the glass. He always breaks the glass.” Notice that there is a relationship between the two sentences: the word he is related to the word Armen. The machine uses NLP to identify this relationship.

A **concept** is something **implied in a sentence but not actually stated**. This is trickier because it involves **matching ideas rather than the specific words** present in the sentence.

## Emotion detection and sentiment analysis are not the same thing

Although emotions and sentiment deal with feelings rather than facts or actions, distinguishing between them can help an AI better understand a sentence.

**Emotion detection** identifies distinct human emotion types.

For example, you can determine if the emotion being expressed is anger, happiness, or fear after reading a user's rating and comments in an online customer satisfaction survey.

AI can be trained to classify emotions. Identifying the right emotional token can make a big difference when an AI system is reading a social media post or a customer service chat, in which different emotions significantly change the meaning of a sentence.

**Sentiment analysis** isn’t a specific emotion —at least, not as computer scientists use the term. Instead, it’s a measure of the strength of an emotion. 

You can think of sentiment as a sliding scale between positive and negative, with neutral in the middle. 

Sentiment analysis is a means of assessing if data is positive, negative, or neutral.

## Human language makes classification challenging

Here’s an old-fashioned riddle:

> Why does your nose run and your feet smell?

Human language is full of terms that are **vague or have double meanings**. This is called a classification problem. In the riddle, run and smell each have two meanings.

- "A runny nose" means you have a cold and you need a tissue to wipe your nose. 
- "A smelly foot" means that your  foot has an unpleasant odor.  
It might only take you a moment to understand the joke, but an AI system might have difficulty classifying its elements. Consider these examples:

- You can ship a box by train.
- When a building burns down, it burns up.
- You can fill in a form by filling it out.
- A wise guy is not the same as a wise person.

**Classification can be more difficult for an AI system than identifying tokens** because so much of classification depends on the **context** in which a sentence is contained. Compare I went to the docks to ship my box to I went to the station to ship my box. Both sentences indicate where a box’s travel begins, but neither specifies how it will travel. An AI system must associate the word ship with either the word station or docks, and then relate that association with the right concept: either train or boat.

How does an AI system deal with this problem? After **ingesting several thousand instances** in which shipping from a dock results in boat travel, while shipping from a station leads to shipping by rail, the AI system **identifies the frequency in which places and kinds of travel are linked**. Gradually, the system gets better at classification and makes fewer mistakes. However, as with humans, an AI system’s classification will never be 100% perfect. (That’s why well-designed AI systems give not only a response, but also a confidence value.)

## Chatbot structure
If you’ve clicked a Chat button online and encountered a non-human chatbot, you might have noticed two things:

- When you ask a clear question that relates to the website’s purpose, like asking a shopping chatbot “How can I get a refund?” it usually gives you a response related to your question.
- When you ask a question that’s unclear or unrelated to the website’s purpose, like asking the shopping chatbot “Got any tickets for sister?” the response will be more along the lines of “I’m sorry, I didn’t understand your question.”
This happens because the chatbot is programmed to answer only limited questions about a particular subject.

Even with this limitation, chatbots are useful in fields ranging from retail sales to immediate care medicine. You can engage with a chatbot online at any time. A chatbot is always ready for your question (even if it cannot answer you). Some institutions use chatbots often because they don’t just broadcast ads like TV commercials. **When people engage with them, chatbots listen, and they answer repetitive questions that a business would otherwise need to pay humans to handle**.

Chatbots work with small data. This means their scale is much more limited. A movie chain’s chatbot might need to answer questions only about movie titles, locations, and times, while a more general AI that searches social media might need to answer broad questions about what millions of people think. AI researchers say that chatbots “snack on small data”.

### A chatbot has a “frontend” and a “backend”

The frontend of a chatbot is the messaging channel. The frontend interacts with the person who’s asking questions, both listening (or reading) and speaking (or presenting text).

The backend of a chatbot is where the hard work takes place. The backend operates application logic and has enough memory to remember earlier parts of a conversation as dialog continues.

## A chatbot’s backend does the hard work of understanding and responding

Suppose part of a chatbot’s job is to help you when you've lost or forgotten your password. You might enter, “How do I reset my password,” which would be easy for the chatbot to handle. To describe that action, a chatbot programmer might think in terms, such as:

> IF question = “How do I reset my password”
> THEN reply = “Here’s how to create a new password”

But humans are not always easy to understand. They might ask or type:

- “How come I can’t log into my account?!”
- “I forgot my @#$ password!”
- “It says my password is wrong.”
- “fergot paswrd”

This tells you that one chatbot response (“This is how to create a new password”) can be triggered by a large number of user queries (including all those listed, and more).

This is a great job for algorithms called **classifiers**. Classifiers can **map many different ways of asking a question to a very small set of answers**. How small? Some retail chatbots respond to hundreds of different questions with only five or six possible answers. Questions the chatbots can’t answer are sent to human customer service representatives.

## A chatbot backend usually includes three parts: intents, entities, and dialog

Chatbots understand a question by breaking it into parts and relating those parts to things in its memory. A chatbot’s goal is to identify what are called entities and intents, then use what it’s found to trigger a dialog. What do these terms mean?

### Intent

An intent is a purpose: **the reason why a user is contacting the chatbot**. Think of it as something like a verb: a kind of action. A user might intend to file a complaint, to ask for directions, to speak to a salesperson.

An institution might have many customer or member intents that it would like a chatbot to handle. Suppose you’ve been hired to help create a chatbot for a restaurant chain. One possible intent would be to find out when the restaurants are open. You might first interview a person who previously handled many forms of this question on the phone. Then, your task would be to list all the different ways a caller might ask about when the restaurants are open. The following table provides many examples of possible user inputs that map to this kind of intent.

![image](https://github.com/user-attachments/assets/6bdfe2e0-994a-43af-bd41-24acde3e64e0)

If a user asks, “What are the hours for the Austin location?”, then providing **business hours is the intent and Austin is the entity**. A chatbot needs a full list of entities in order to be helpful.

### Dialog

A dialog is a **flowchart—an IF / THEN tree structure that illustrates how a machine will respond to user intents.** A dialog is what the machine replies after a human asks a question. Even if a human uses run-on sentences, poor grammar, chat messaging expressions, and so on, artificial intelligence allows the NLP to understand well enough to provide a response.

The dialog represents each possible word or phrase a user might enter, the matched response for the chatbot, and the many possible subsequent replies a user might make next. That’s too much for an ordinary flowchart to show (you might need three or four dimensions!), so chatbot software **condenses each moment of the conversation into a node**. A node contains a statement by the chatbot and a long, expandable list of possible replies.

Planning this flowchart would be an adventure! You’d need to assign a reply to every possible user input after the chatbot’s greeting. For the restaurant hours example, all possible questions about a restaurant’s hours would lead to a single reply. This would continue for the next question (perhaps for the restaurant’s address), and so on. A large number of possible questions would be mapped onto a small number of possible answers until the conversation is ended. (Spoiler: It helps that today’s virtual assistants already have been trained on Wikipedia, so they know, for example, the difference between a wise man and a wise guy.)

## Intents, entities, and dialogs make quick work for NLP

In a conventional computer, the program code is stupendously large but wouldn’t handle intents, entities, and dialogs very well. A conventional computer would need a separate IF / THEN line for many thousands of ways a question might be asked. Unless a human were to match one of those lines perfectly, the computer would fail. 

But an AI system’s combination of NLP with intents, entities, and dialog can make quick work of this. NLP analyzes sentence components, then uses processes like passage and evidence scoring to classify the sentence components against possible chatbot responses. The result is that when a human user asks a question, the AI system provides the answer with the highest confidence.

Consider Staples, the office supply company that features the Easy button. Suppose a customer asks for a chatbot on Staples’ website. The chatbot’s frontend receives the customer’s inquiry, then forwards it to the chatbot’s backend. There, IBM Watson Assistant runs the NLP to understand the human’s intentions regarding ordering a product or tracking a shipment.

If the customer says, “I want to reorder black pens,” the chatbot figures out what that means. Then, it uses cognitive services to pull up the customer’s purchase history. Within seconds, the system is helping the customer buy more pens.

The Staples chatbot only knows five things, but it knows them well. It supports online sales 24 hours a day, giving callers round-the-clock service while leaving human staff free to do other work. This is valuable for both the customers and the business.


# DAY - 11

## Convolutional neural networks

> An AI system uses a convolutional neural network (CNN) to analyze images

Digital photos and videos build **images with millions of pixels**—tiny dots that, taken together, give the human eye the illusion of a two-dimensional image. Each pixel has one of several million brightness and color levels that can be expressed in numbers. A computer (including the camera in a cellphone) can look at each of those pixels and input or output its numbers, either recording (input) or displaying (output) an image. But it’s one thing to display an image and another, much more complicated thing to analyze it.

The **problem is caused by those millions of pixels** with their color and brightness levels. They **add up to numbers so large that they would overwhelm most AI systems**. (Research scientists say that too much data can “flood” a system.) To get around this, research scientists have devised **a clever way to analyze only small parts of an image at a time**. This process, called a **convolutional neural network, or CNN**, makes it possible for visual recognition systems to identify things in an image, as in facial recognition.

Here’s a clear and concise **note summary** from the video transcript you shared about **Convolutional Neural Networks (CNNs):**

---

### **Convolutional Neural Networks (CNNs)**

#### **Introduction**

* CNN = *Convolutional Neural Network*
* A **deep learning** technique specialized in **pattern recognition**.
* Inspired by how humans recognize objects easily, even in abstract or simplified forms (e.g., drawing of a house).
* Used to help computers perform **object identification** tasks.

---

#### **Key Components of CNN**

1. **Artificial Neural Network (ANN) Basics**

   * Consists of **multiple layers**:

     * Each layer takes input → transforms it → passes output to next layer.

2. **Convolution Layers & Filters**

   * **Filters**: Small matrices (e.g., 3×3 blocks) applied over the image.
   * They **slide over (convolve)** the image to detect patterns.

     * Example: Look for right angles, lines, or edges in an image.
   * Each filter produces a **numeric array** showing how closely parts of the image match the pattern.

3. **Pooling**

   * Combines results from multiple filters.
   * Helps reduce dimensionality while preserving important information.

---

#### **Example: Identifying a House**

* Image = collection of pixels.
* CNN uses filters to detect:

  * **First layers**: Basic patterns (e.g., straight lines, edges).
  * **Deeper layers**: More abstract features (e.g., windows, doors, roofs).
  * **Final layers**: Complete objects (e.g., house vs apartment vs skyscraper).

---

#### **Applications of CNN**

* **Optical Character Recognition (OCR)** – Reading handwritten text.
* **Visual recognition** – Object detection & classification.
* **Facial detection** – Identifying faces in images/videos.
* **Medical imagery** – Analyzing scans for diagnosis.
* **Visual search** – Finding images similar to a query image.

## Generative adversarial networks

> A visual recognition system can use a generative adversarial network (GAN) to create new drawings and photos

Recall that an AI system can analyze a photo using a neural network to identify a photo. But what about AI systems themselves that create imaginary photos?

You might have heard of computer-generated **“deep fakes”**: computer-generated images that look like they were taken from real life. You might have also seen drawings or handwriting created by AI “artists”. How does a system do this? One way is by **pitting two convolutional neural networks (CNNs) against each other in a “contest” called a generative adversarial network, or GAN**. In effect, the CNNs battle each other until one of them becomes pretty good at creating art.


### **Generative Adversarial Networks (GANs)**

#### **Introduction**

* GAN stands for Generative Adversarial Network.
* It involves two AI models competing against each other, hence the term *“adversarial.”*
* GANs are a type of unsupervised learning where the system supervises itself.

---

#### **Key Components**

1. **Two Sub-Models**

   * **Generator**: Creates fake samples (e.g., fake images of flowers).
   * **Discriminator**: Decides whether a sample is real (from actual data) or fake (created by the generator).

2. **Adversarial Process**

   * The generator tries to fool the discriminator with realistic fake data.
   * The discriminator tries to correctly identify real versus fake data.
   * The winner (correct model) remains unchanged, while the loser updates its model.
   * This process repeats many times until the generator’s fake data is so convincing that the discriminator can no longer distinguish it from real data.

---

#### **Example: Training GAN with Flower Images**

1. The discriminator is first trained on real flower images to learn their attributes such as color, shading, and shape.
2. Once the discriminator becomes effective at distinguishing real flowers, the generator starts creating fake flower images.
3. The discriminator evaluates whether each image is real or fake.
4. Based on the result:

   * If the discriminator correctly detects the fake, the generator updates to create better fakes.
   * If the generator successfully fools the discriminator, the discriminator updates to improve its detection ability.
5. This adversarial loop continues until the generator produces highly convincing images.

---

#### **Key Characteristics**

* Zero-sum game: In each iteration, either the generator or the discriminator improves.
* Both models progressively enhance each other’s performance.

---

#### **Applications of GANs**

* **Image Generation**: Creating fake faces, objects, animals.
* **Video Frame Prediction**: Predicting the next frame in a video sequence, useful in surveillance.
* **Image Enhancement**: Upscaling low-resolution images to higher resolution.
* **Encryption**: Developing secure encryption algorithms using the iterative GAN process.

---

#### **Implementation Note**

* Both the generator and discriminator are often implemented using Convolutional Neural Networks (CNNs) because CNNs are effective at identifying patterns in image data.

---

#### **Key Takeaways**

* GANs involve a generator and discriminator working against each other to improve over time.
* They produce highly realistic data and have applications beyond images, including video and encryption.
* Many iterations are needed for the generator to achieve mastery in creating convincing outputs.

## Computer vision has many useful applications

- The IBM Maximo visual inspection system can be equipped with video cameras on drones. It can not only detect problems, like cracks in a suspension bridge, but identify which problems (cracks, in this example) are dangerous and should be repaired.
- Spotting a dangerous but difficult-to-detect flaw in an airplane’s wing
- Monitoring water flow across a dairy farm to ensure it doesn’t reach nearby food crops
- Counting the number of people in an unruly crowd
- Classifying animal and plant populations to measure biodiversity in a forest
- Performing lip-reading for people who cannot hear or speak


## What are AI Ethics?

> Every day people interact with AI systems, but how much do they trust them?

### The age of AI

Artificial intelligence is everywhere. It **powers the navigation apps** that help you find the most efficient or most eco-friendly route. It **drives the search engines** that help you find the most relevant information. It helps **doctors reach more accurate diagnoses and develop more optimal treatment plans**. It improves **weather forecasting**, enabling you to better prepare for significant weather events. In conjunction with sensors and satellites, it can collect **data about the environment** that helps scientists better understand and make predictions about our changing world. AI can make life easier and safer by helping people make more informed decisions, connecting them with the right information they need at the right time, and finding patterns or efficiencies that they might not otherwise know about.

But AI also has the **potential to do harm**. For example, AI can be used when determining **who gets a loan, who gets accepted to college or selected for a job, how employees are compensated, or even the lengths of prison sentences**. In the context of AI, harm doesn’t necessarily have to do with physical harm. The harm can be less obvious, taking the **form of inequity, discrimination, or exclusion**. And this harm can be subtle because people may not always know when they are interacting with AI or when and how AI may be influencing decisions about them.

### Can AI be trusted?
As AI is increasingly embedded in everyday life, it is vital that people can trust AI. Practitioners infuse trust into AI systems with AI ethics. **AI ethics is a multidisciplinary field that investigates how to optimize AI’s beneficial impact while reducing unintended or adverse outcomes.** There are five pillars of AI ethics: **fairness, robustness, explainability, transparency, and privacy**. These pillars are focus areas that help make AI trustworthy.


### **Trustworthy AI – The Five Pillars**

#### **Introduction**

* Trustworthy AI is essential for businesses to ensure ethical, reliable, and responsible AI systems.
* There are **five fundamental pillars** commonly discussed: **Fairness, Robustness, Privacy, Explainability, and Transparency**.
* This is a fast-evolving field, and definitions or practices may change over time.

---

#### **1. Fairness**

* Ensures AI models do not behave in a biased or discriminatory way.
* Bias may originate **before model building**, such as in biased datasets.
* Focus areas:

  * Detect and address bias in training data.
  * Ensure the model does not systematically favor or disadvantage any group.
  * Groups may vary by use case (e.g., age, gender, ethnicity).
* Goal: Prevent unfair advantages or disadvantages.

---

#### **2. Robustness**

* Ensures models perform reliably under **exceptional conditions** or **changing environments**.
* Addresses challenges like:

  * **Data drift**: Changes in input data patterns over time.
  * Examples: Changing customer behavior during events like the COVID-19 pandemic.
* Goal: Monitor and maintain model performance, understanding how and why behavior changes.

---

#### **3. Privacy**

* Protects data, models, and insights throughout their lifecycle.
* Includes:

  * Data protection during model building, testing, validation, and deployment.
  * Control over who owns and accesses insights derived from models.
* Goal: Comply with data privacy regulations and ensure user data safety.

---

#### **4. Explainability**

* Provides clear reasoning for model decisions.
* Examples:

  * Why was someone approved or rejected for a loan?
  * Why was one applicant selected for a job over another with similar qualifications?
* Important for both **end users** and **decision-makers**.
* Goal: Make AI behavior understandable and justifiable.

---

#### **5. Transparency**

* Allows full inspection and understanding of all aspects of a model.
* Includes:

  * Who built the model?
  * What data and algorithms were used?
  * What tools and packages were applied?
  * Who approved and validated it?
* Analogy: Similar to a food label showing ingredients, nutritional facts, and manufacturing details.
* Goal: Provide easy access to all facts about a model for accountability.

---

#### **Key Challenge**

* Implement these pillars **systematically**, independent of the tools or deployment environments used.
* Ensuring consistency and adherence across the entire AI lifecycle is critical for building trust.


## **Fairness**

What’s meant by fairness in AI systems? This story looks at a large national banking company and some of the issues with fairness that must be considered and dealt with when deploying AI systems.

As the company gets ready to deploy an AI system to help them identify high-value candidates in their promotion pool, Priscilla notices that most of the candidates have a college degree. They launch a review of the system to understand what is driving the result and find that the system is biased. This story covers basic concepts of bias within the context of fairness, and how bias can enter a system. The story also provides a set of questions for you to think about.

Jordan has taken a new role as the banking company’s Chief Data Officer. Before Jordan joined, the company had been testing a new AI system, the first of its kind, to help their PeopleOps group **identify high-value candidates in their promotion pool for the current year**.

The system seems to be working, so the team shares the promotion list with Priscilla, the Director of People Operations. First, Priscilla looks at the list alongside the candidates’ demographic information. As she goes through the list, something catches her attention. Priscilla notices that **a disproportionate number of the promoted candidates have a college degree**. She is surprised because she had specifically recommended an employee without a college degree who was already performing the next-level job and who sounded like the perfect fit for a promotion, but that person is not on the promotion list. She calls Jordan to start an investigation.

Jordan asks the data science team to look into Priscilla’s concerns. As a first step, the data science team quickly analyzes the 5-year promotion data used for training the AI system. 

The following table shows a sample of the data they collected:
![image](https://github.com/user-attachments/assets/6fd58cd9-9df9-46ea-ad32-7455157ba17f)

![image](https://github.com/user-attachments/assets/c4384b5a-c03e-48e1-9a13-cd65552dbe62)

By looking at the graph, Jordan notices that historically there were disproportionately more candidates with a college degree than candidates without a college degree in the final candidate pool for promotion. It occurs to them that the system could have a fairness problem resulting in bias based on education.

ordan gets the team together to discuss the issue they found and to plan what they need to do next.

As a first step to address the dilemma, Jordan asks the team if they think there is business value in keeping the AI system. 

Jordan asks, “Do you think we should continue using AI to help with the promotion process?”
After taking a moment to think, Priscilla nods her head.

“If done well and in a way PeopleOps team members can trust, it will save us time and could help us make our promotion process fairer,” says Priscilla.

Jordan, Priscilla, and Nischal all agree that the value of AI is there, if implemented correctly.

The next day, Nischal and the PeopleOps team assemble in the meeting room. Nischal starts the discussion with the term bias.

Jordan asks Nischal, “Can you explain bias in the context of fairness?”  

Nischal agrees and explains, “Bias, in general, is a systematic error, but in the context of fairness, the concern is around unwanted bias. Unwanted bias places some groups or individuals at a systematic advantage and other groups or individuals at a systematic disadvantage.” 

Priscilla asks, “Why would we split the population into groups?”
Nischal explains, “The reason to use groups is simple: to mitigate disparity in the outcome across the groups. In other words, to have an equitable outcome across the groups.”

Nischal continues, “We divide the population into groups based on one or more attributes in the data that could introduce disparity or inequity in the outcome. **The attribute that separates the population into groups is called a protected attribute**. Some generally used protected attributes are race, age, sex at birth, gender identity, and ethnicity. But…there isn’t a defined set of protected attributes.”

Note: For legal and other policy reasons, it is often the case that protected attributes are not maintained in the data set. In these cases, the team might impute them (which is fraught with its own biases) or might use the statistics from demographic data sets using a so-called transfer learning approach. 

“Traditionally, if one group receives a more favorable or advantageous outcome than the other group, bias can enter and cause an unfair result. The group that traditionally receives more favorable outcomes is called the privileged group. The group that traditionally receives fewer to no favorable outcomes is called the unprivileged group. Fairness in AI aims to minimize the impact of unwanted bias on the system,” Nischal concludes.
Nischal points to the graph, “In our data, education is one of the possible protected attributes.”

So, in our promotion application,

- Employees who have a college degree are in the privileged group.
- Employees who do not have a college degree are in the unprivileged group.
- Education, when used to separate the population into groups, introduces disparity, and is the protected attribute.

The team has identified the issue as one of fairness in the AI model. To achieve fairness, unwanted bias needs to be reduced. In AI, bias is a systematic error that, intentionally or not, can influence an AI system in a way that might generate unfair decisions. Bias can be present both in the AI system and in the data used to train and test it. Based on the data that was given to the AI system, bias had crept in and was affecting the system’s results.

### **Question 1**

**Is there a step to analyze the intended and unintended consequences of the application using a design-thinking approach? Do you think that such a step is necessary?**

**Yes**, it is very important to understand both the known and hidden effects of the application to mitigate potential harm.

* One effective way to do this is by considering **layers of effect**:

  * **Primary effects**: Intended impact of the application.
  * **Secondary effects**: Known or predictable unintended impacts.
  * **Tertiary effects**: Potential unpredictable or unforeseen unintended impacts.
* Involving a **diverse and inclusive team** helps identify a wider range of potential impacts, including possible harms, and strengthens the design-thinking process.

---

### **Question 2**

**Which attribute in this story’s dataset has the potential to introduce unwanted bias?**

**The "Employee education" attribute**

* This attribute may encode systemic inequalities or reflect societal biases, leading to unintended discrimination if not handled carefully.

---

### **Question 3**

**Is there a way to mitigate bias at every stage of the AI lifecycle (from development to deployment)? How would you do it?**

**Yes**, bias mitigation should be applied at all stages of the AI lifecycle.

* **Key practices:**

  * Work with a **diverse and inclusive team** to better identify potential bias.
  * Use **high-quality data** that is relevant, accurate, complete, and representative to reduce bias in model training.
  * Apply **bias detection and mitigation tools** during development and after deployment to monitor performance.
  * Continuously detect, measure, and address bias in production to proactively remediate issues.

---

### **Question 4**

**What are the ways observed bias can be dealt with?**

There are several ways to address observed bias:

1. **Investigate** where and why the model exhibits bias.
2. **Review and fix** issues in the data and data labeling process.
3. **Retrain the model** with corrected or additional data if necessary.
4. Implement ongoing **monitoring** and evaluation of model behavior to catch future bias.


## What is robustness?

Next, you’ll look at how AI systems can be protected from attacks. **Adversarial robustness refers to an AI model’s ability to resist being fooled**. Teams are constantly working to make AI systems more impervious to irregularities and attacks. This story looks at what a medical diagnostic company can do to safeguard AI systems against attacks. 

A medical diagnostic company is getting close to launching **an application to detect lung cancer**. Then they hear that another healthcare app start-up is facing a lawsuit for misdiagnosed cases. The company’s technical leadership team pulls everyone together to learn what an **adversarial attack** is and to assess the risk of their system being attacked. In this story, you’ll learn what an adversarial attack is, the different types of attacks, how they can happen, and what teams need to think about to protect their AI systems.

The launch date for an AI-based application to aid in the detection of lung cancer using chest x-rays is quickly approaching. Charlie, the Chief Technology Officer, is reviewing the plan with her team

**Adversarial attacks** are intentionally carried out on AI systems to accomplish a malicious end goal by taking advantage of AI system vulnerabilities.

“An adversarial attack aims to negatively impact the system performance, exploit data used, and corrupt the model logic. The one who takes advantage of the AI system vulnerabilities to accomplish their motive is called an **adversary**.

- Get access to patient’s personal information like age, gender, race, medical history, identification information, and finance information.
- Add malicious x-ray samples to the input training data and make the system learn from the malicious data that is in favor of the adversary.
- Predict the presence of disease among significant numbers of patients who don’t have the disease or vice-versa. This also reduces the system performance.
- Intentionally add noise to the data so more patients will fall under the “Disease Detected” category.
- Learn the company’s AI model and related data. Develop a similar model and use for own purpose.
- Send malicious and corrupted x-ray images to the company’s AI application and recreate training data based on the system’s response.

Prashant notes, “There are multiple ways for an adversary to achieve the goals mentioned above including the following: 

Getting access to the training data and learning the data distribution
Having permission to modify the data used for training and testing the AI system
Having access to the model code and parameters
Corrupting the user’s data and sending the modified data to the deployed AI system”
“The adversary might not necessarily experiment with only one way to enter a system. Instead, it is likely that the adversary might use a combination of different ways to attack a system. Also, the ways adversaries attack can vary from attack to attack.”

Prashant then sums up by saying, “Given the goals and needs, adversarial attacks can occur either at the model training or after the model deployment.”

### The issue of poisoning

Prashant pauses here and thinks about potential attacks that can happen to their AI system.
“As you can see, poisoning can happen in the following ways during the model training phase:

- Injecting malicious samples into the training data
- Updating features and labels of the training data
-  AI model architecture, parameters, and logic”
“There are several impacts of the poisoning attack. One is when the deployed AI model becomes sensitive to the malicious data's specific pattern.”

### The issue of evasion

Evasion can happen after the model deployment phase in the following ways:

- Sending malicious test samples to the deployed model
- Corrupting test data sent to the deployed model

The team now understands how an AI model can be corrupted by poisoning and evasion and why it is important that the model is robust. The team is now ready to go make the system robust against adversarial attacks. 

Here’s a clean and professional version of your answers without emojis:

---

### **Question 1**

**Is the source of the data used by the AI model important to know?**

Yes, the source of the data is very important to know. AI models rely heavily on data to learn, so the quality of data directly impacts the trustworthiness of the AI system. It is essential to understand:

* How the data was collected.
* Who has access to the data.
* How the data has been used in the past.

This helps ensure the data is relevant, accurate, unbiased, and compliant with regulations.

---

### **Question 2**

**Is it better to use public data? Should the data be screened before it is used?**

It is neither inherently better nor worse to use public data. While public data can be useful, it must be **carefully vetted** before use to ensure:

* Accuracy and relevance.
* Completeness and representativeness.
* Absence of biases or harmful content.
  Screening the data before using it is critical to prevent introducing issues into the AI model.

---

### **Question 3**

**How would you monitor for attacks on the deployed model?**

A good strategy is to use tools that proactively monitor the deployed model for attacks and anomalies. For example, **IBM AI Robustness 360** can help detect and mitigate adversarial attacks and maintain model integrity. Continuous monitoring ensures the model remains secure and performs as expected in real-world conditions.

## What is explainability?

An AI system is explainable when everyday people, who do not have any special training in AI, can understand how and why the system came to a particular prediction or recommendation. Explainability is like showing your work on a math problem so that everyone can see the steps you took to get your answer.

After deploying an **AI-based product recommendation system**, an online retailer notices that customers want to understand why and how they are getting the product recommendations. The Data Science team dives into how to explain the models in different ways that are relevant for different users. The team also generates a set of questions to think about when solving this kind of problem. 

**Interpretability** is the degree to which an observer can understand the cause of a decision. It is the success rate that humans can predict the result of an AI output, while **explainability** goes a step further and looks at how the AI system arrived at a result.

Thanks to Luan, Olivia and Clara are ready to get to work on increasing the explainability of their AI model. The team now understands the importance of making sure that everyone, not just customers, can understand how and why AI systems generate particular predictions or recommendations.

### **Question 1**

**What is a way for the team to be aware of who is involved in development and deployment of the AI system and each of their roles?**

It is important to define roles clearly before starting work on the project, ideally after the business discussion stage. This includes identifying all team members and assigning responsibilities for each stage of the AI lifecycle. Building a **diverse and inclusive team**, including a variety of stakeholders, helps create systems that are more trustworthy and better reflect different perspectives.

---

### **Question 2**

**How can the team gather the type of explanations each persona will aim to get from the model?**

The team should first define the **personas** and identify what types of explanations each persona needs. This step involves understanding the goals, technical expertise, and expectations of different users. After defining the personas, the team can work together to decide how to provide these explanations in a way that is meaningful and does not detract from the overall user experience.

---

### **Question 3**

**Which explanation method will better match with each persona’s expectation?**

Explanation methods vary depending on the consumer of the explanation. To select the best method, it is important to understand each persona’s needs, technical background, and goals. Using a **design thinking approach** can help the team consider these factors and tailor explanation methods to meet the expectations of each persona effectively.


## What is transparency?

When an AI system is transparent, information is shared about what data it collects, how it uses and stores the data, and who has access to the data. When effectively implemented, transparency in AI is like lifting the hood of a car to let everyone see how the different parts of the engine work together. 


One of the largest banking companies in the world is testing an AI system to **automate their mortgage approval process**. Abi, the Business Owner, is impressed with the system so far and has asked to see key details about the system to better understand it. Abi's request makes the team realize that the key details are not clearly documented and therefore not straightforward to understand. This story covers what the team did to increase transparency and create a standard for documenting details, from the sources of data to how the system makes predictions and recommendations.

Transparency is one of the keys to creating trustworthy AI. Going forward, the team will document what data is collected, how it is used, how it is stored, and who has access to the data. This transparency will allow questions about the AI system to be answered easily and quickly.

### **Question 1**

**How can you determine the roles involved in developing and deploying the AI system?**

It is important to define roles clearly before starting the project, preferably right after the business discussion stage. This involves identifying all team members who will participate in development and deployment and specifying their responsibilities. Building a **diverse and inclusive team**, including a variety of stakeholders, contributes to creating more trustworthy AI systems by incorporating different perspectives and expertise.

---

### **Question 2**

**What types of facts can each role involved in the process provide?**

Defining the roles and mapping them to the facts they can provide is an important step toward achieving transparency. Examples include:

* **Business Owner**: Facts about the purpose and intended use of the system.
* **Data Scientists / Data Engineers / AI Model Lead / Chief Risk Officer**: Facts about the data, such as data sources, data quality, and fairness evaluations.
* **Software Developers**: Technical details about the system implementation.
* **Operations Team**: Information about model deployment and monitoring processes.
  This mapping exercise ensures that all necessary facts are gathered from the appropriate experts.

---

### **Question 3**

**Who do you think should be responsible for ensuring compliance with the whole process?**

Ensuring compliance is a complex task that typically requires oversight by a **Chief Compliance Officer** or another senior leader. However, compliance is a **shared responsibility**. A team or committee often works together to enable and enforce compliance across the organization. Everyone involved in the AI lifecycle has a role to play in supporting compliance through their specific contributions and actions.


## What is privacy?

Data is the heart of AI. Because AI ingests so much data to learn, identify patterns, and make predictions or recommendations, it must prioritize and safeguard the privacy of all of the data it comes into contact with.

How important is privacy? This story looks at a large educational institution and how privacy must be considered when developing AI systems.

An educational institution seeks to expand its reach by launching an online global campus. After analyzing the market, the institution’s business development team proposes creating personalized learning experiences for each student. The team believes that the most efficient way to do this is to use an AI model to create personalized learning curricula. The institution’s data science team is brought in to create a proof-of-concept AI model using sample data collected from the institution’s existing learning portal data.

When the proof of concept is complete, the business development and data science teams gather to review the AI model.

Personal information (PI)  is any information relating to an identified or identifiable individual, like a name or postal code. Sensitive personal information (SPI) is information that, if compromised, could be misused to significantly harm or inconvenience an individual, like a bank account number or birth date. How would you categorize these types of information?

Because the machine learning models that drive AI often need to be trained using personal or sensitive information, it is critical that AI systems prioritize and safeguard privacy. If a model is trained using personal or sensitive information without any privacy controls applied, then it could be vulnerable to breaches or attacks.

Well, there are many privacy controls that can be applied to fortify AI against potential breaches of personal or sensitive data. Two that occur during model training are **model anonymization** and **differential privacy**. One that occurs after model training is **data minimization**.

- The goal of model anonymization is to anonymize the training data with minimal accuracy loss. After all, if the model is trained on anonymous data, then the model itself is anonymous and there is little risk to any personal data used during training.

- In differential privacy, random noise is added during model training to reduce the impact of any single individual on the model’s outcomes and to give a guarantee that an individual in the training data set could not be identified.

- Data minimization means that only data that is needed is being collected. This control helps prevent privacy breaches by limiting the amount of personal data that is collected in the first place and by ensuring that collected data is only as granular as needed. For example, data minimization might mean that you collect only an individual’s zip code instead of their full address, or only their year of birth instead of their full birth date.

Now, the team knows the concepts of personal and sensitive information and understands that information that seems harmless may be able to be used to identify individuals. Securing personal data and the AI model is important for both the company and their users.

### **Question 1**

**How could making the model publicly available introduce a risk to privacy?**

If an attacker gains access to the model, they may be able to infer which individuals were included in the training data. This type of attack is known as a **membership inference attack**. To reduce this risk, it is essential to apply AI privacy controls such as:

* Anonymizing the training data.
* Adding noise to the data.
  These techniques make it much harder for attackers to identify individuals from the model’s behavior.

---

### **Question 2**

**If there is risk involved with using personal information, is it still worth using?**

Yes, personal information can still be used to train models when it is necessary and appropriate. However, privacy-preserving techniques must be applied to the data to protect individuals. This includes methods such as:

* Data anonymization.
* Differential privacy.
* Data encryption during processing and storage.
  These measures help ensure responsible use of personal information while mitigating risks.

---

### **Question 3**

**What is another example of data minimization?**

Examples of data minimization include:

* Using an individual’s **industry** instead of their company or job title.
* Using an **area code** instead of a full telephone number.
* Asking **yes/no questions** instead of collecting detailed information (e.g., “Did you graduate from high school? Yes/No” instead of asking for the high school’s name or graduation date).

Data minimization helps reduce the collection of unnecessary personal details, lowering privacy risks.


# DAY - 12

## Introduction to Artificial Intelligence and Machine Learning

### **What is Artificial Intelligence (AI)?**

Artificial Intelligence (AI) refers to the ability of a machine to perform tasks that would typically require human intelligence. These tasks include decision-making, object recognition, language processing, and problem-solving. The field of AI is vast, encompassing a range of subfields such as:

* **Machine Learning (ML)**: A method of data analysis that automates analytical model building.
* **Natural Language Processing (NLP)**: Allows machines to understand, interpret, and generate human language.
* **Robotics**: Involves creating intelligent machines capable of performing tasks autonomously.
* **Computer Vision**: Enables machines to interpret and make decisions based on visual data.
* **Expert Systems**: Simulate the decision-making ability of a human expert.

### **Types of AI**

AI can be broadly classified into two categories:

1. **Narrow AI (Weak AI):**

   * Specialized in performing a single task.
   * Examples: Siri, Google Assistant, self-driving cars.
   * Narrow AI is the most common form of AI today.

2. **General AI (Strong AI):**

   * Has the ability to understand, learn, and apply intelligence across a wide range of tasks, similar to human cognitive abilities.
   * Currently, this level of AI does not exist, and research is ongoing.

### **What is Machine Learning (ML)?**

Machine Learning (ML) is a subset of AI that enables systems to learn from data, identify patterns, and make decisions with minimal human intervention. The key feature of ML is that the system improves its performance as it is exposed to more data, without needing to be explicitly programmed.

### **How AI and ML are Different**

* **AI** is the broader concept of machines performing tasks intelligently, while **ML** is the specific method or technique used to enable machines to learn from data.
* **AI** can include reasoning, problem-solving, and decision-making, whereas **ML** focuses specifically on algorithms and statistical methods to make predictions or classifications based on data.

### **Applications of AI and ML**

* **Healthcare:** AI models can predict diseases, assist in diagnostic procedures, and personalize treatment recommendations.
* **Finance:** AI algorithms power fraud detection, risk assessments, and customer service chatbots.
* **Transportation:** AI-driven autonomous vehicles and route optimization.
* **Entertainment:** AI algorithms personalize recommendations on platforms like Netflix and YouTube.
* **Retail:** AI powers recommendation engines, demand forecasting, and dynamic pricing strategies.

### **AI and ML Tools and Technologies**

* **Programming Languages:** Python, R, Java, C++
* **Libraries/Frameworks:**

  * **TensorFlow**, **Keras** (Deep Learning)
  * **Scikit-learn** (Traditional ML)
  * **PyTorch** (Deep Learning)
  * **Pandas**, **NumPy** (Data Handling)
  * **Matplotlib**, **Seaborn** (Data Visualization)
* **Development Environments:** Jupyter Notebooks, Google Colab, IBM Watson Studio
* **Cloud Platforms:** AWS, Google Cloud, Azure for ML model deployment



# DAY - 13

## Understanding Machine Learning Concepts in Depth with Examples

### **What is Machine Learning (ML)?**

Machine learning is a method of data analysis that automates analytical model building. It is a branch of AI based on the premise that systems can learn from data, identify patterns, and make decisions with minimal human intervention. Unlike traditional software that follows explicit instructions, ML systems improve their performance through experience (i.e., exposure to more data).

### **Types of Machine Learning**

1. **Supervised Learning**

In supervised learning, the model is trained on labeled data, where both input and output are provided. The algorithm learns by comparing its output with the true results and adjusts accordingly.

* **Example:** Spam Email Classification

  * **Input:** Features (e.g., words in the email, sender, etc.)
  * **Output:** Label (spam or not spam)
  * **Goal:** Train a model that can correctly classify new emails as spam or non-spam.

* **Algorithms:** Linear Regression, Logistic Regression, Decision Trees, Support Vector Machines (SVM), Neural Networks.

2. **Unsupervised Learning**

In unsupervised learning, the model is trained on unlabeled data. The goal is to find hidden patterns or intrinsic structures in the data. It is often used for clustering or dimensionality reduction.

* **Example:** Customer Segmentation

  * **Input:** Customer data (age, spending, purchase history)
  * **Output:** No labels, but groups of similar customers.
  * **Goal:** Group customers into clusters to target with personalized marketing.

* **Algorithms:** K-Means Clustering, Hierarchical Clustering, PCA (Principal Component Analysis).

3. **Reinforcement Learning**

Reinforcement learning involves an agent that learns by interacting with its environment. The agent takes actions to maximize cumulative rewards. This type of learning is inspired by behavioral psychology.

* **Example:** Training an AI to play a game (e.g., chess, Go)

  * **Input:** Game state (board configuration)
  * **Action:** Move a piece or take a turn
  * **Reward:** Win or loss of the game
  * **Goal:** Maximize the chances of winning by taking actions that lead to higher rewards.

* **Algorithms:** Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods.

### **Key Concepts in Machine Learning**

1. **Model Evaluation**

   * Evaluating the performance of a model is crucial. Common metrics include:

     * **Accuracy:** Percentage of correct predictions.
     * **Precision and Recall:** Particularly useful for imbalanced datasets.
     * **F1-Score:** Harmonic mean of precision and recall.
     * **Confusion Matrix:** A table showing true positives, false positives, true negatives, and false negatives.

2. **Overfitting vs Underfitting**

   * **Overfitting:** The model learns too much from the training data, including noise, leading to poor performance on unseen data.
   * **Underfitting:** The model is too simple and cannot capture the underlying patterns in the data.

   **Solution:** Regularization techniques (e.g., L1, L2 regularization), cross-validation.

3. **Feature Engineering**

   * The process of selecting and transforming features to improve model performance. This includes techniques like scaling, encoding categorical variables, and extracting new features from raw data.

4. **Gradient Descent**

   * A method of optimization used to minimize the error or cost function in models like linear regression and neural networks. It helps adjust model parameters to minimize the loss function.

5. **Bias-Variance Tradeoff**

   * **Bias:** Error due to overly simplistic models (underfitting).
   * **Variance:** Error due to overly complex models (overfitting).
   * Balancing bias and variance is crucial for building robust models.

### **ML Example: Predicting Employee Salaries using Supervised Learning**

1. **Dataset:** Employee Salaries dataset with features such as Age, Gender, Education Level, Job Title, Years of Experience and Salary.
2. **Problem:** Predict the salary of an employee based on the input features.
3. **Model:** Linear Regression
4. **Training:** The model learns the relationship between features (input) and target variable (price).
5. **Evaluation:** Use Mean Squared Error (MSE) to evaluate how well the model performs on the test data.

**`Project Link:`** **[PayPredict: Employee Salary Prediction and Management System](https://github.com/Diya050/PayPredict)**

### **Challenges in Machine Learning**

* **Data Quality:** ML models require clean, labeled data to learn from. Poor data can result in poor performance.
* **Computational Power:** Some machine learning models, especially deep learning models, require significant computational resources.
* **Model Interpretability:** Many complex models (e.g., deep neural networks) are hard to interpret, making it difficult to explain decisions made by the model.

### **Conclusion**

Machine learning is a powerful tool for creating models that can make predictions and decisions based on data. By understanding the concepts, algorithms, and practical applications, you can build and implement your own ML solutions to solve real-world problems.


## **Day 14 — Data Preprocessing and Exploration**

Data preprocessing is the most important phase of any Machine Learning project. It ensures that raw data is cleaned, transformed, and prepared so that ML models can understand and learn from it effectively.

---

## **1. What is Data Preprocessing?**

Data preprocessing involves converting raw, messy, incomplete, or inconsistent data into clean and usable form.
It includes:

* Handling missing values
* Removing duplicates
* Converting data types
* Encoding categorical features
* Scaling numerical features
* Feature selection and reduction

This step directly impacts model accuracy and performance.

---

## **2. Steps in Data Preprocessing**

### **✔ Step 1: Import the Data**

Using tools like **Pandas**, you load the dataset and check its structure.

```python
import pandas as pd
df = pd.read_csv("data.csv")
df.head()
```

---

### **✔ Step 2: Handle Missing Values**

Common methods:

* **Remove missing rows/columns**
* **Fill values** using mean, median, mode
* **Predict missing values** using ML algorithms

Example:

```python
df['age'].fillna(df['age'].mean(), inplace=True)
```

---

### **✔ Step 3: Handle Duplicates**

```python
df.drop_duplicates(inplace=True)
```

---

### **✔ Step 4: Encode Categorical Variables**

ML models need numbers, not text.

* **Label Encoding** for categories
* **One-Hot Encoding** for nominal features (e.g., Gender, City)

```python
df = pd.get_dummies(df, columns=['gender', 'city'])
```

---

### **✔ Step 5: Feature Scaling**

Used to normalize wide-range numerical values.

* StandardScaler
* MinMaxScaler
* Normalization

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[['salary', 'experience']] = scaler.fit_transform(df[['salary', 'experience']])
```

---

### **Step 6: Feature Selection & Dimensionality Reduction**

Helps remove irrelevant or redundant features.

Methods:

* Correlation matrix
* Mutual information
* PCA (Principal Component Analysis)

---

## **3. Exploratory Data Analysis (EDA)**

Before modeling, you must **understand the dataset** deeply.

### **✔ Descriptive Statistics**

```python
df.describe()
```

---

### **✔ Data Visualization**

* Histogram → distribution
* Boxplot → detect outliers
* Heatmap → correlations

Example:

```python
import seaborn as sns
sns.heatmap(df.corr(), annot=True)
```

---

## **4. Why Preprocessing Matters?**

* Improves model accuracy
* Reduces bias and noise
* Helps model learn meaningful patterns
* Prevents overfitting/underfitting
* Leads to better predictions

---

## **5. Mini-Exercises (Hands-on)**

### **Task 1:**

Load any dataset (Iris, Titanic, etc.), identify missing values, and fix them.

### **Task 2:**

Perform one-hot encoding for at least 2 categorical columns.

### **Task 3:**

Create a correlation heatmap of the dataset using Seaborn.

---

## **6. Real-World Uses of Data Preprocessing**

* Medical diagnosis prediction
* Fraud detection
* Stock market forecasting
* Recommendation systems
* Natural language processing (text cleaning)

