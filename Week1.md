# DAY 1

## Introduction to Artificial Intelligence

### What is AI?
Artificial intelligence (AI) refers to the ability of a machine to learn patterns and make predictions. AI does not replace human decisions; instead, AI adds value to human judgment. 

In its simplest form, artificial intelligence is a field that combines computer science and robust datasets to enable problem-solving.

### What is the difference between AI and augmented intelligence?
> When learning about artificial intelligence, you’ll come across the term augmented intelligence. Both terms share the same objective, but have different approaches. 
- Augmented intelligence has a modest goal of helping humans with tasks that are not practical to do. For example, “reading” 1000 pages in an hour.
- In contrast, artificial intelligence has a lofty goal of mimicking human thinking and processes.
However, it’s important to note that AI today is not mature enough to perform independent tasks such as diagnosing cancer.

#### Real-Life Analogy: My Commute

Driving to work involves three forms of intelligence:
- Human intelligence – Used for basic driving tasks like steering and checking mirrors.
- Artificial Intelligence: Took over on the highway via the car’s self-driving mode—managing speed, lane control, and safe distance with no human input.
- Augmented Intelligence: Kicked in with features like collision detection and blind spot warnings, which assisted the driver without replacing them.
It made me realize: AI replaces, but Augmented AI empowers.

#### Key Differences:

* **AI**: Replaces human decision-making to perform tasks such as reasoning, problem-solving, and communication.
* **Augmented Intelligence**: Enhances human abilities through technology—like screen readers or driver-assist tools.

#### Strengths Matrix:

| Strengths           | Humans    | Machines (AI)          |
| ------------------- | --------- | ---------------------- |
| Data Processing     | Limited   | Fast and high-volume   |
| Creativity          | Strong    | Weak                   |
| Emotional Skills    | Excellent | Lacking                |
| Accuracy in Repeats | Can vary  | Consistent and precise |

**Conclusion**: The best results often come from **both AI and humans working together**—a balance found in **Augmented Intelligence**.

### What continues to drive the development of AI? 

As computing power and algorithms become more powerful and data volumes increase, companies will adopt new use cases for AI technologies. Companies will embed smart systems into their applications to drive innovation and efficiencies, enhance employee experience, automate tasks, decrease costs, and improve revenue.

### What does AI do?

> Artificial intelligence machines (researchers call them “AI services”) don’t think. They calculate. <br>

They represent some of the newest, most sophisticated calculating machines in human history. Some can perform what’s called machine learning as they acquire new data. Others, using calculations arranged in ways inspired by neurons in the human brain, can even perform deep learning with multiple levels of calculations.

#### Task

> Imagine you are given the job to sort items in the produce department at a grocery store. You realize that there are dozens of products and very little time to sort them manually. How could you use artificial intelligence, machine learning, and deep learning to help with your work?

#### Artificial Intelligence:

To separate the stone fruits, berries, and tropical fruits, you could create **a programmed rule in the format of if-else statements**. This allows the machine to recognize what is on the label and route it to the correct basket. 

A programmed rule might look something like this:

```
if berries_is_on_label:
   route_items_to_center_basket()

else:
   redirect_item_to_main_basket()
```
 
Artificial intelligence makes this process more efficient.<br><br>
![image](https://github.com/user-attachments/assets/51b8ccdf-4d3f-4828-8b57-97707acbf912)

#### Machine Learning:

To improve the performance of the machine, you expose it to more data to ensure that the **machine is trained on numerous characteristics of each type of fruit**. The more data you provide for the algorithm, the better the model gets. By providing more data and adjusting parameters, the machine minimizes errors by repetitive guess work.


![image](https://github.com/user-attachments/assets/7f19b92c-793e-4067-a9e9-10e1bf91bcd9)

#### Deep Learning:

The grocery store has expanded its produce selection to include more varieties such as nectarines and plums (stone fruits), blackberries and cranberries (berries), and mangoes and star fruit (tropical fruits). In addition, the products now come in different sizes, shapes, and colors. What makes deep learning different?

Deep learning models **eliminate the need for feature extractions**. For your work in the product department, you decide to use algorithms based on deep learning to sort fruit by removing the need to define what each product looks like. Feature extraction is built into the process without human input. Once you have provided the deep learning model with dozens of fruit pictures, it processes the images through different layers of neural networks. The layers can then learn an implicit representation of the raw data on their own.

![image](https://github.com/user-attachments/assets/6e5ef1f6-9452-4bc9-8d3a-7e88c40088ae)


### How do AI services calculate?

- **Analyze**: AI services can take in (or “ingest”) enormous amounts of data. They can apply mathematical calculations to analyze data, sorting and organizing it in ways that would have been considered impossible only a few years ago.
- **Prediction**: AI services can use their data analysis to make predictions. They can, in effect, say, “Based on this information, a certain thing will probably happen.”

### What predictions can AI make?

Most people have a love-hate relationship with the autocorrect feature on phones or computers. What’s happening when you enter a misspelled word? And how does the machine know to suggest a better spelling?

Simply put, the software analyzes what you’ve typed so far and predicts a likely correction. Your phone or computer (or its online service) has more than just a dictionary of correct spellings. It has a huge library of phrases that humans use in certain contexts on many subjects. So, when you enter a word that’s not in its dictionary, it begins analyzing and predicting and suggests the word you need. Predictions aren’t always accurate. But if they’re correct often enough, they’re useful and can save you time.

- **Human Language**: Online chatbots use natural language processing (NLP) to analyze poorly typed or spoken questions, then predict which answers to give on topics ranging from shipping or business hours to merchandise and sizes.
- **Vision Learning**: AI helps doctors identify serious diseases based on unusual symptoms and early-warning signs, and it reads speed limit and stop signs as it guides cars through traffic.
- **Fraud Detection**: AI analyzes patterns created when thousands of bank customers make credit card purchases, then predicts which charges might be the result of identity theft.

### How is AI evolving?

Computer scientists have identified three levels of AI based on predicted growth in its ability to analyze data and make predictions. They call these levels:
- Narrow AI
- Broad AI
- General AI

![image](https://github.com/user-attachments/assets/4c3a5922-d301-4804-93ca-43fc607de2f4)

#### Narrow AI:

- Narrow AI is focused on addressing a single task such as predicting your next purchase or planning your day. 
- Narrow AI is scaling very quickly in the consumer world, in which there are a lot of common tasks and data to train AI systems. For example, you can buy a book with a voice-based device. 
- Narrow AI also enables robust applications, such as using Siri on an iPhone, the Amazon recommendation engine, autonomous vehicles, and more. Narrow AI systems like Siri have conversational capabilities, but only if you stick to the script.

#### Broad AI:

- Broad AI is a midpoint between Narrow and General AI. 
- Rather than being limited to a single task, Broad AI systems are more versatile and can handle a wider range of related tasks. 
- Broad AI is focused on integrating AI within a specific business process where companies need business- and enterprise-specific knowledge and data to train this type of system. 
- Newer Broad AI systems predict global weather, trace pandemics, and help businesses predict future trends.

#### General AI:

- General AI refers to machines that can perform any intellectual task that a human can. 
- Currently, AI does not have the ability to think abstractly, strategize, and use previous experiences to come up with new, creative ideas as humans do, such as inventing a new product or responding to people with appropriate emotions. And don't worry, AI is nowhere near this point.

There might be another level, known as artificial superintelligence (ASI) that could appear near the end of this century. Then machines might become self-aware! Even then, no levels of AI are expected to replace or dominate you. Instead, scientists hope AI will extend humans’ ability to lead richer lives.


# DAY - 2

## What are the three eras of computing?

### The Era of Tabulation

For centuries, people have struggled to understand the meaning that’s hidden in large amounts of data. After all, it’s one thing to estimate how many trees grow in a million square miles of forest. It’s something else to classify what species of trees they are, how they cluster at different altitudes, and what could be built with the wood they provide. That information can be difficult to extract from a very large amount of data. 

> Because it's hard to see without help, scientists call this **dark data**. It’s information without a structure: just a huge, unsorted mess of facts.

- To sort out unstructured data, humans have created many different calculating machines. Over 2000 years ago, tax collectors for Emperor Qin Shihuang used the abacus—a device with beads on wires—to break down tax receipts and arrange them into categories. From this, they could determine how much the Emperor should spend on building extensions to the Great Wall of China.
- In England during the mid-1800s, Charles Babbage and Ada Lovelace designed (but never finished) what they called a “difference engine” designed to handle complex calculations using logarithms and trigonometry. Had they built it, the difference engine might have helped the English Navy build tables of ocean tides and depth soundings that could guide English sailors through rough waters.
- By the late 1880s, people were thinking about how to develop faster systems to record data. Herman Hollerith, inspired by train conductors using holes punched in different positions on a railway ticket to record traveler details, invented the recording of data on a machine-readable punched card. Hollerith’s cards were used for the 1890 US Census, which finished months ahead of schedule and under budget. Later versions of tabulating machines had broad applications in business, such as financial accounting and data processing.

The word to remember across those twenty centuries is tabulate. Think of **tabulation as “slicing and dicing”** data to give it a structure, so that people can uncover patterns of useful information. You tabulate when you want to get a feel for what all those columns and rows of data in a table really mean.

Researchers call these centuries the **Era of Tabulation**, a time when machines helped humans sort data into structures to reveal its secrets.


### The Era of Programming

- During the turmoil of World War II, a new approach to dark data emerged: the Era of Programming. Scientists began building electronic computers, like the Electronic Numerical Integrator and Computer (ENIAC) at the University of Pennsylvania, that could run more than one kind of instruction (today we call those “programs”) in order to do more than one kind of calculation. ENIAC, for example, not only calculated artillery firing tables for the US Army, it worked in secret to study the feasibility of thermonuclear weapons.

- This was a huge breakthrough. Programmable computers guided astronauts from Earth to the moon and were reprogrammed during Apollo 13’s troubled mission to bring its astronauts safely back to Earth.

- You’ve grown up during the **Era of Programming**. It even drives the phone you hold in your hand. But the dark data problem has also grown. Modern businesses and technology generate so much data that even the finest programmable supercomputer can't analyze it before the “heat-death” of the universe. Electronic computing is facing a crisis.

### The Era of AI

The history of artificial intelligence dates back to philosophers thinking about the question, "What more can be done with the world we live in?" This question lead to discussions and the very beginning of many ideas about the possibilities involving technology. 

Since the advent of electronic computing, there are some important events and milestones in the evolution of artificial intelligence to know about. Here's an overview to get started.

- **Turing Machine:** Alan Turing publishes Computing Machinery and Intelligence. In the paper, Turing—famous for helping to break the Nazis’ Enigma code during World War II—proposes to answer the question "can machines think?" and introduces the Turing Test to determine if a computer can demonstrate the same intelligence (or the results of the same intelligence) as a human.
- **Dartmouth Conference - Birth of AI:** John McCarthy coins the term "artificial intelligence at the first-ever AI conference at Dartmouth College. Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the **Logic Theorist**, the first-ever running AI software program. McCarthy would go on to invent the Lisp language.
- **1st AI Winter:** 1st AI Winter caused by high expectations from end users and reduced funding.
- **2nd AI Winter:** 2nd AI Winter caused by unmet expectations and computing power.
- **Deep Blue:** IBM Deep Blue beats then world champion Garry Kasparov in a chess match (and rematch).
- **IBM Watson** beats champions Ken Jennings and Brad Rutter at the US game show called Jeopardy!
- **DeepMind's AlphaGo** program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Google bought DeepMind for a reported USD 400 million in 2014.
- IBM unveils **Project Debater**, the first AI system capable of engaging with humans on complex topics in a live debate.

![image](https://github.com/user-attachments/assets/bafc3bbe-9327-48aa-a85d-ce8a84777af3)

#### The Era of AI began one summer in 1956

Early in the summer of 1956, a small group of researchers, led by John McCarthy and Marvin Minsky, gathered at Dartmouth College in New Hampshire. There, at one of the oldest colleges in the United States, they launched a revolution in scientific research and coined the term “artificial intelligence”.

The researchers proposed that “every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.” They called their vision “artificial intelligence” and they raised millions of dollars to achieve it within 20 years. During the next two decades, they accomplished tremendous things, creating machines that could prove geometry theorems, speak simple English, and even solve word problems with algebra. 

> For a short time, AI was one of the most exciting fields

#### But then came winter

By the early 1970s, it became clear that the problem was larger than researchers imagined. There were fundamental limits that no amount of money and effort could solve.

- **Limited Calculating Power:** Today, it is important for a computer to have enough processing power and memory. Every ad you see for companies like Apple or Dell emphasizes how fast their processors run and how much data they can work with. But in 1976, scientists realized that even the most successful computers of the day, working with natural language, could only manipulate a vocabulary of about 20 words. But a task like matching the performance of the human retina might require millions of instructions per second, at a time when the world’s fastest computer could run only about a hundred. By the early 1970s, it became clear that the problem was larger than researchers imagined. There were fundamental limits that no amount of money and effort could solve.

- **Limited Information Storage:** Even simple, commonsense reasoning requires a lot of information to back it up. But no one in 1970 knew how to build a database large enough to hold even the information known by a 2-year-old child.

As these issues became clear, the money dried up for The First Winter of AI.

#### The weather was rough for half a century

It took about a decade for technology and AI theory to catch up, primarily with **new forms of AI called “expert systems”**. These were limited to specific knowledge that could be manipulated with sets of rules. They worked well enough—for a while—and became popular in the 1980s. Money poured in. Researchers invested in tremendous mainframe machines that cost millions of dollars and occupied entire floors of large university and corporate buildings. It seemed as if AI was booming once again.

But soon the needs of scientists, businesses, and governments outgrew even these new systems. Again, funding for AI collapsed.

#### Then came another AI chill

In the late 1980s, the boom in AI research cooled, in part, because of the **rise of personal computers**. Machines from Apple and IBM, sitting on desks in people’s homes, grew more powerful than the huge corporate systems purchased just a few years earlier. Businesses and governments stopped investing in large-scale computing research, and funding dried up.

Over 300 AI companies shut down or went bankrupt during The Second Winter of AI.

#### Now, the forecast is sunny

In the mid-1990s, almost half a century after the Dartmouth research project, the Second Winter of AI began to thaw. Behind the scenes, **computer processing finally reached speeds fast enough for machines to solve complex problems**.

At the same time, the public began to see AI’s ability to play sophisticated games.

- In 1997, IBM’s Deep Blue beat the world’s chess champion by processing over 200 million possible moves per second.
- In 2005, a Stanford University robot drove itself down a 131-mile desert trail.
- In 2011, IBM’s Watson defeated two grand champions in the game of Jeopardy!

Today, AI has proven its ability in fields ranging from cancer research and big data analysis to defense systems and energy production. Artificial intelligence has come of age. AI has become one of the hottest fields of computer science. Its achievements impact people every day and its abilities increase exponentially. The Two Winters of AI have ended!

## Structured, semi-structured, or unstructured data: What are the differences?

### A look at the types of data

Data is raw information. Data might be facts, statistics, opinions, or any kind of content that is recorded in some format. This could include voices, photos, names, and even dance moves!

Data can be organized into the following three types.

Data is raw information. Data might be facts, statistics, opinions, or any kind of content that is recorded in some format. This could include voices, photos, names, and even dance moves!

Data can be organized into the following three types.

- **Structured data** is typically categorized as quantitative data and is highly organized. Structured data is information that can be organized in rows and columns. Perhaps you've seen structured data in a spreadsheet, like Google Sheets or Microsoft Excel. Examples of structured data includes names, dates, addresses, credit card numbers, stock information.
- **Unstructured data**, also known as dark data, is typically categorized as qualitative data. It cannot be processed and analyzed by conventional data tools and methods. Unstructured data lacks any built-in organization, or structure. Examples of unstructured data include images, texts, customer comments, medical records, and even song lyrics.
- **Semi-structured data** is the “bridge” between structured and unstructured data. It doesn't have a predefined data model. It combines features of both structured data and unstructured data. It's more complex than structured data, yet easier to store than unstructured data. Semi-structured data uses metadata to identify specific data characteristics and scale data into records and preset fields. Metadata ultimately enables semi-structured data to be better cataloged, searched, and analyzed than unstructured data. An example of semi-structured data is a video on a social media site. The video by itself is unstructured data, but a video typically has text for the internet to easily categorize that information, such as through a hashtag to identify a location.

> The importance of unstructured data is rapidly increasing. Recent projections(opens in a new tab) indicate that 95% of businesses prioritize unstructured data management.

Are you wondering why this is important? Why would anyone want to search through a mountain of data such as social media posts? 

Here are just some of the many people and organizations that might need to do this:

- A sneaker designer looking for new trends
- Governments searching for possible terrorists
- Pandemic experts trying to anticipate disease outbreaks
- Financial institutions preparing for good times or a recession

> Experts estimate that about 80% of all the data in today’s world is unstructured. It contains so many variables and changes so quickly that no conventional computer program can learn much from it.

### Analyzing unstructured data

Now, imagine a programmable computer trying to extract meaning from billions of data like this! What kind of program would someone write that could sort out every eventuality among the clutter? How would someone build a long enough list of keywords to find anything useful? Unstructured data hides answers to disease prevention, criminal activity, stock markets—almost every aspect of civilization today. Without those answers, people and organizations cannot make useful predictions or recommendations.

But AI can shed light on unstructured data! AI uses new kinds of computing—some modeled on the human brain—to rapidly give dark data structure, and from it, make new discoveries. AI can even learn things—by itself—from the data it manages and teach itself how to make better predictions over time. This is the Era of AI, and it changes everything!

# DAY - 3

## Is machine learning the answer to the unstructured data problem?

### How does machine learning approach a problem?

If AI doesn’t rely on programming instructions to work with unstructured data, how does AI do it? Machine learning can analyze dark data far more quickly than a programmable computer can. To see why, consider the problem of finding a route through big city traffic using a navigation system. It’s a dark data problem because solving it requires working with not only a complicated street map, but also with changing variables like weather, traffic jams, and accidents. Let’s look at how two different systems might try to solve this problem.

- **Programmable Computer:** <br>
Researchers might upload onto the computer a complete database of all possible routes through the city. This is an enormous collection of structured data.

Then they would have to add much more data describing current weather and traffic conditions. This would have to be revised every few minutes for the entire city!

Then they might use a programmable computer to search through the data until it finds a route from start to finish. The entire project would require astronomical resources and time, if it could be accomplished at all!

- **AI with Machine Learning:** <br>

The machine learning AI would treat the problem like climbing a tree. The system would try a route, as if starting at the base of the trunk. Upon reaching a branch, the system would then fork in one direction and continue doing so until it reached either a dead end or the desired destination.

It would do this over and over again, then compare successful routes to identify the shortest one. Although the work sounds repetitious, it requires fewer resources and can be completed more quickly.

### The machine learning process is entirely different

The machine learning process has advantages:

- It doesn’t need a database of all the possible routes from one place to another. It just needs to know where places are on the map.
- It can respond to traffic problems quickly because it doesn’t need to store alternative routes for every possible traffic situation. It notes where slowdowns are and finds a way around them through trial and error.
- It can work very quickly. While trying single turns one at a time, it can work through millions of tiny calculations.

But machine learning has two more advantages that programmable computers lack:

- **Machine learning can predict**. You know this already. A machine can determine, “Based on traffic right now, this route is likely to be faster than that one.” It knows this because it compared routes as it built them.
- **Machine learning learns!** It can notice that your car was delayed by a temporary detour and adjust its recommendations to help other drivers.

### Machine learning uses probabilistic calculation

There are two other ways to contrast classical and machine learning systems. One is deterministic and the other is probabilistic.

Let’s dig in and see what these two words mean.

- For a **deterministic** system, there must be an enormous, predetermined structure of routes—a gigantic database of possibilities from which the machine can make its choice. If a certain route leads to the destination, then the machine flags it as “YES”. If not, it flags it as “NO”. This is basically binary thinking: on or off, yes or no. This is the essence of a computer program. The answer is either true or false, not a confidence value.

- Machine learning is **probabilistic**. It never says “YES” or “NO”. Machine learning is analog (like waves gradually going up and down) rather than binary (like arrows pointing upward and downward). Machine learning constructs every possible route to a destination and compares them in real time, including all the variables such as changing traffic. So, a machine learning system doesn’t say, “This is the fastest route.” It says something like, “I am 84% confident that this route will get you there in the shortest time.” You might have seen this yourself if you’ve traveled in a car with an up-to-date GPS navigation system that offers you two or three choices with estimated times.

### Machine learning enables a rich partnership between technology and humans

AI systems and humans excel at different things. For example, you, as a person, might excel at imagining possibilities, while AI excels at pinpointing patterns.

### Does common sense make sense?

It turns out that in fields, ranging from medicine and education to social studies and government, the best decisions are made using a **balance of human and machine strengths**. But remember, there is another elusive but vital capability that must also be considered: common sense. You might know people with strong common sense and understand its value. You also might have seen or read output from machines that makes no sense. Yet, there’s a contribution to be made from both sides.

Common sense draws on many **complex generalizations mixed with compassion and abstractions**. At this time, only humans can use common sense well. The problem is that common sense is often tainted with bias that can distort your judgment. AI systems can balance this. As long as AI systems are provided and trained with unbiased data, they can make recommendations that are free of bias. A partnership between humans and machines can lead to sensible decisions. 


## Three common methods of machine learning

Machine learning solves problems in three ways: 

- Supervised learning
- Unsupervised learning
- Reinforcement learning


### Supervised learning

**Supervised learning** is about providing AI with enough examples to make accurate predictions. 

All supervised learning algorithms need **labeled data**. Labeled data is data that is grouped into samples that are tagged with one or more labels. In other words, applying supervised learning requires you to tell your model:

- What the key characteristics of a thing are, also called **features**
- What the thing actually is

For example, the information might be drawings and photos of animals, some of which are dogs and are labeled “dog”. The machine will learn by identifying a pattern for “dog”. When the machine sees a new dog photo and is asked, “What is this?”, it will respond, “dog”, with high accuracy. This is known as a classification problem.

### Unsupervised learning

In **unsupervised learning**, a person feeds a machine a large amount of information, asks a question, and then the machine is left to figure out how to answer the question by itself.

For example, the machine might be fed many photos and articles about dogs. It will classify and cluster information about all of them. When shown a new photo of a dog, the machine can identify the photo as a dog, with reasonable accuracy.

Unsupervised learning is helpful when you don't know how to classify data. For example, imagine you work for a banking institution and you have a large set of customer financial data. You don't know what type of groups or categories to organize the data. Here, an unsupervised learning algorithm could find natural groupings of similar customers in a database, and then you could describe and label them. 

This type of learning has the **ability to discover similarities and differences in information**, which makes it an ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition.

### Reinforcement learning

Reinforcement learning is a machine learning model similar to supervised learning, but the algorithm isn’t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes is reinforced to develop the best recommendation for a given problem. The foundation of reinforcement learning is rewarding the “right” behavior and punishing the “wrong” behavior.

You might be wondering, what does it mean to "reward" a machine? Good question! Rewarding a machine means that you give your agent positive reinforcement for performing the "right" thing and negative reinforcement for performing the "wrong" things. 

As a machine learns through trial and error, it tries a prediction, then compares it with data in its corpus. 

- Each time the comparison is positive, the machine receives positive numerical feedback, or a **reward**.
- Each time the comparison is negative, the machine receives negative numerical feedback, or a **penalty**.
Over time, a machine’s predictions will grow to be more accurate. It accomplishes this automatically based on feedback, rather than through human intervention.

## How will machine learning transform human life?

Perhaps, 25 years from now, General AI is expected to emerge. AI researcher Nick Bostrom defines this superintelligence as, “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.” You’re likely to see General AI appear in your lifetime. General AI will enable supersmart bots and technologies to link AI with the Internet of Things through “embodied cognition”. This will give machines the ability to interact in human-like ways as they work alongside humans.

- **AI Everywhere:** AI will move into all industries, from finance, to education, to healthcare. AI will increase productivity and enable new opportunities.
- **Deeper Insights:** New technologies will sense, analyze, and understand things never before possible.
- **Engagement Reimagined:** New forms of human-machine interaction and emerging technologies, such as conversational bots, will transform how humans engage with each other and with machines.
- **Personalization:** Machine interactions will be personalized for you, with new levels of detail and scale.
- **Instrumented Planet:** Billions of sensors generating exabytes of data will open new possibilities for improving Earth’s safety, sustainability, and security.

What’s beyond these wonders? Humans, devices, and robots might exist as a collective “digital brain” that anticipates human needs, makes predictions, and provides solutions. Farther in the future, we might trust the digital brain to do things on our behalf across a broad spectrum of endeavors!

# DAY - 4

## How do machines learn?

Supervised learning requires that an AI system ingest structured data. Unsupervised learning and reinforcement learning require a system to develop its own structure either by analyzing large amounts of data (unsupervised learning) or by trial-and-error (reinforcement learning).

## Classical machine learning

Classical machine learning began in the 1950s. AI systems learned by ingesting data and getting better at recognizing patterns. The AI systems could predict things like the distance between points or the intensity of values.

Like all machine learning, the classical form depends on algorithms. Recall that algorithms are mathematical expressions that output a result. C**lassical machine learning uses a small number of algorithms in a relatively simple arrangement**. Sometimes machine learning algorithms are **binary**, which means that they output one of only two values. Typical binary results might be a 1 or a 0, a YES or a NO, and a TRUE or a FALSE.

Other classical learning algorithms are more complicated. For example, their result might be represented as a position on a multidimensional graph rather than “this point” or “that point”. Here are three typical algorithms that are used in classical computing:

- Decision tree
- Linear regression
- Logistic regression

## Decision tree
A decision tree is a supervised learning algorithm. It operates like a flowchart. You can think of a flowchart as an upside-down decision tree. The flowchart has a root node (where the flowchart begins), branches that connect to internal nodes, and more branches that connect to leaf nodes.

![image](https://github.com/user-attachments/assets/0de07ede-2a9a-4e01-8ccd-311f48231afd)

## Linear regression
Linear regression is another type of algorithm. It relates to data that might be graphed as a straight line. For example, a business might believe that more advertising spending leads to better sales. This could be graphed as a series of dots that form a rising straight line, as depicted here.

![image](https://github.com/user-attachments/assets/dc54c1b1-d3cc-4c47-874e-de64e73ca376)

As suggested in the chart, as advertising increases, so do sales. There are many possible outcomes (different amounts of advertising lead to different amounts of sales), but the **change rises on the graph in a straight line**.

The situation is more complicated if a company’s actual sales show different data for different products, at different locations, on different dates, and so on. With a large number of variables and instances, the graph becomes a mass of dots that don’t arrange into a straight line at all. Without adjustment, resulting graph is too general to help a business make a good decision. That’s where linear regression can help. Linear regression can learn all the variables, then calculate a reasonably accurate prediction of how advertising will impact sales at some time and location in the future. In effect, **linear regression resolves the mass of dots into a “most likely”** line that can be used for simple prediction.

## Logistic regression
In some situations, a relationship does not fall in a straight line. Sometimes a system uses values that require a specific, limited kind of outcome, such as something between 0 and 1 (or NO and YES). In this situation, a graph can form what’s called a sigmoid function, or an S-shaped curve, as shown in the accompanying example. For any set of variables, the outcome (which is a point on the S-curve) falls between 0 and 1.

![image](https://github.com/user-attachments/assets/c52d2512-fc05-40e3-8a16-e7c6bcf5dbcd)

Here’s a real-world example. Refer to the previous graph. Let’s say you want to know how many hours you should study in order to pass an exam. You have the number of study hours and passing or failing status for 10 other students. “Hours of studying” is a varying amount, in this case, between 1 and 5 hours. Passing the exam is a matter of NO or YES (either FAIL or PASS).

If you plot these two factors together as a logistical regression, you get an S curve in which 0 hours of study results in a very low chance of passing, while 5.5 hours results in a very high chance. As shown in the chart, the variable “Hours of studying” is along the x-axis. The values along the y-axis represent the values for the variable “Probability of passing exam”.

Here’s another way to understand the graph: it predicts that studying at least 4 hours gives you a very good chance of passing the course.

### Comparing linear and logistic regressions

Linear and logistic regressions are useful in the following ways:

- A **linear** regression answers a question such as “If this increases by X, how much will Y increase?”
- A **logistic** regression answers a question such as “If this increases by X, will the value of Y be closer to 0 or 1?”

## Classical machine learning is not obsolete
Classical machine learning can be outperformed, at some tasks, by newer methods that are part of the deep learning ecosystem. But there are still reasons to use classical machine learning. These include:

- **Work with structured data:**  Classical machine learning is used mostly with structured data from databases, such as hours studied compared to grades earned.
- **Lower expense to operate:** Classical machine learning requires less computing power than deep learning ecosystems. They can run on less expensive computers with less powerful processors, which lowers the price for smaller businesses, communities, or healthcare systems that share time on them in pay-as-you-go arrangements.
- **Easier to interpret:** Deep networks are so complex that even AI researchers don’t entirely understand what’s going on inside. As a result, AI researchers are not always able to determine when deep network systems are producing invalid outputs. Compared to these mysteries, classical results can be easier to debug, and to test for accuracy and lack of bias.

## The deep learning ecosystem
