# DAY 1

## Introduction to Artificial Intelligence

### What is AI?
Artificial intelligence (AI) refers to the ability of a machine to learn patterns and make predictions. AI does not replace human decisions; instead, AI adds value to human judgment. 

In its simplest form, artificial intelligence is a field that combines computer science and robust datasets to enable problem-solving.

### What is the difference between AI and augmented intelligence?
> When learning about artificial intelligence, you’ll come across the term augmented intelligence. Both terms share the same objective, but have different approaches. 
- Augmented intelligence has a modest goal of helping humans with tasks that are not practical to do. For example, “reading” 1000 pages in an hour.
- In contrast, artificial intelligence has a lofty goal of mimicking human thinking and processes.
However, it’s important to note that AI today is not mature enough to perform independent tasks such as diagnosing cancer.

#### Real-Life Analogy: My Commute

Driving to work involves three forms of intelligence:
- Human intelligence – Used for basic driving tasks like steering and checking mirrors.
- Artificial Intelligence: Took over on the highway via the car’s self-driving mode—managing speed, lane control, and safe distance with no human input.
- Augmented Intelligence: Kicked in with features like collision detection and blind spot warnings, which assisted the driver without replacing them.
It made me realize: AI replaces, but Augmented AI empowers.

#### Key Differences:

* **AI**: Replaces human decision-making to perform tasks such as reasoning, problem-solving, and communication.
* **Augmented Intelligence**: Enhances human abilities through technology—like screen readers or driver-assist tools.

#### Strengths Matrix:

| Strengths           | Humans    | Machines (AI)          |
| ------------------- | --------- | ---------------------- |
| Data Processing     | Limited   | Fast and high-volume   |
| Creativity          | Strong    | Weak                   |
| Emotional Skills    | Excellent | Lacking                |
| Accuracy in Repeats | Can vary  | Consistent and precise |

**Conclusion**: The best results often come from **both AI and humans working together**—a balance found in **Augmented Intelligence**.

### What continues to drive the development of AI? 

As computing power and algorithms become more powerful and data volumes increase, companies will adopt new use cases for AI technologies. Companies will embed smart systems into their applications to drive innovation and efficiencies, enhance employee experience, automate tasks, decrease costs, and improve revenue.

### What does AI do?

> Artificial intelligence machines (researchers call them “AI services”) don’t think. They calculate. <br>

They represent some of the newest, most sophisticated calculating machines in human history. Some can perform what’s called machine learning as they acquire new data. Others, using calculations arranged in ways inspired by neurons in the human brain, can even perform deep learning with multiple levels of calculations.

#### Task

> Imagine you are given the job to sort items in the produce department at a grocery store. You realize that there are dozens of products and very little time to sort them manually. How could you use artificial intelligence, machine learning, and deep learning to help with your work?

#### Artificial Intelligence:

To separate the stone fruits, berries, and tropical fruits, you could create **a programmed rule in the format of if-else statements**. This allows the machine to recognize what is on the label and route it to the correct basket. 

A programmed rule might look something like this:

```
if berries_is_on_label:
   route_items_to_center_basket()

else:
   redirect_item_to_main_basket()
```
 
Artificial intelligence makes this process more efficient.<br><br>
![image](https://github.com/user-attachments/assets/51b8ccdf-4d3f-4828-8b57-97707acbf912)

#### Machine Learning:

To improve the performance of the machine, you expose it to more data to ensure that the **machine is trained on numerous characteristics of each type of fruit**. The more data you provide for the algorithm, the better the model gets. By providing more data and adjusting parameters, the machine minimizes errors by repetitive guess work.


![image](https://github.com/user-attachments/assets/7f19b92c-793e-4067-a9e9-10e1bf91bcd9)

#### Deep Learning:

The grocery store has expanded its produce selection to include more varieties such as nectarines and plums (stone fruits), blackberries and cranberries (berries), and mangoes and star fruit (tropical fruits). In addition, the products now come in different sizes, shapes, and colors. What makes deep learning different?

Deep learning models **eliminate the need for feature extractions**. For your work in the product department, you decide to use algorithms based on deep learning to sort fruit by removing the need to define what each product looks like. Feature extraction is built into the process without human input. Once you have provided the deep learning model with dozens of fruit pictures, it processes the images through different layers of neural networks. The layers can then learn an implicit representation of the raw data on their own.

![image](https://github.com/user-attachments/assets/6e5ef1f6-9452-4bc9-8d3a-7e88c40088ae)


### How do AI services calculate?

- **Analyze**: AI services can take in (or “ingest”) enormous amounts of data. They can apply mathematical calculations to analyze data, sorting and organizing it in ways that would have been considered impossible only a few years ago.
- **Prediction**: AI services can use their data analysis to make predictions. They can, in effect, say, “Based on this information, a certain thing will probably happen.”

### What predictions can AI make?

Most people have a love-hate relationship with the autocorrect feature on phones or computers. What’s happening when you enter a misspelled word? And how does the machine know to suggest a better spelling?

Simply put, the software analyzes what you’ve typed so far and predicts a likely correction. Your phone or computer (or its online service) has more than just a dictionary of correct spellings. It has a huge library of phrases that humans use in certain contexts on many subjects. So, when you enter a word that’s not in its dictionary, it begins analyzing and predicting and suggests the word you need. Predictions aren’t always accurate. But if they’re correct often enough, they’re useful and can save you time.

- **Human Language**: Online chatbots use natural language processing (NLP) to analyze poorly typed or spoken questions, then predict which answers to give on topics ranging from shipping or business hours to merchandise and sizes.
- **Vision Learning**: AI helps doctors identify serious diseases based on unusual symptoms and early-warning signs, and it reads speed limit and stop signs as it guides cars through traffic.
- **Fraud Detection**: AI analyzes patterns created when thousands of bank customers make credit card purchases, then predicts which charges might be the result of identity theft.

### How is AI evolving?

Computer scientists have identified three levels of AI based on predicted growth in its ability to analyze data and make predictions. They call these levels:
- Narrow AI
- Broad AI
- General AI

![image](https://github.com/user-attachments/assets/4c3a5922-d301-4804-93ca-43fc607de2f4)

#### Narrow AI:

- Narrow AI is focused on addressing a single task such as predicting your next purchase or planning your day. 
- Narrow AI is scaling very quickly in the consumer world, in which there are a lot of common tasks and data to train AI systems. For example, you can buy a book with a voice-based device. 
- Narrow AI also enables robust applications, such as using Siri on an iPhone, the Amazon recommendation engine, autonomous vehicles, and more. Narrow AI systems like Siri have conversational capabilities, but only if you stick to the script.

#### Broad AI:

- Broad AI is a midpoint between Narrow and General AI. 
- Rather than being limited to a single task, Broad AI systems are more versatile and can handle a wider range of related tasks. 
- Broad AI is focused on integrating AI within a specific business process where companies need business- and enterprise-specific knowledge and data to train this type of system. 
- Newer Broad AI systems predict global weather, trace pandemics, and help businesses predict future trends.

#### General AI:

- General AI refers to machines that can perform any intellectual task that a human can. 
- Currently, AI does not have the ability to think abstractly, strategize, and use previous experiences to come up with new, creative ideas as humans do, such as inventing a new product or responding to people with appropriate emotions. And don't worry, AI is nowhere near this point.

There might be another level, known as artificial superintelligence (ASI) that could appear near the end of this century. Then machines might become self-aware! Even then, no levels of AI are expected to replace or dominate you. Instead, scientists hope AI will extend humans’ ability to lead richer lives.


# DAY - 2

## What are the three eras of computing?

### The Era of Tabulation

For centuries, people have struggled to understand the meaning that’s hidden in large amounts of data. After all, it’s one thing to estimate how many trees grow in a million square miles of forest. It’s something else to classify what species of trees they are, how they cluster at different altitudes, and what could be built with the wood they provide. That information can be difficult to extract from a very large amount of data. 

> Because it's hard to see without help, scientists call this **dark data**. It’s information without a structure: just a huge, unsorted mess of facts.

- To sort out unstructured data, humans have created many different calculating machines. Over 2000 years ago, tax collectors for Emperor Qin Shihuang used the abacus—a device with beads on wires—to break down tax receipts and arrange them into categories. From this, they could determine how much the Emperor should spend on building extensions to the Great Wall of China.
- In England during the mid-1800s, Charles Babbage and Ada Lovelace designed (but never finished) what they called a “difference engine” designed to handle complex calculations using logarithms and trigonometry. Had they built it, the difference engine might have helped the English Navy build tables of ocean tides and depth soundings that could guide English sailors through rough waters.
- By the late 1880s, people were thinking about how to develop faster systems to record data. Herman Hollerith, inspired by train conductors using holes punched in different positions on a railway ticket to record traveler details, invented the recording of data on a machine-readable punched card. Hollerith’s cards were used for the 1890 US Census, which finished months ahead of schedule and under budget. Later versions of tabulating machines had broad applications in business, such as financial accounting and data processing.

The word to remember across those twenty centuries is tabulate. Think of **tabulation as “slicing and dicing”** data to give it a structure, so that people can uncover patterns of useful information. You tabulate when you want to get a feel for what all those columns and rows of data in a table really mean.

Researchers call these centuries the **Era of Tabulation**, a time when machines helped humans sort data into structures to reveal its secrets.


### The Era of Programming

- During the turmoil of World War II, a new approach to dark data emerged: the Era of Programming. Scientists began building electronic computers, like the Electronic Numerical Integrator and Computer (ENIAC) at the University of Pennsylvania, that could run more than one kind of instruction (today we call those “programs”) in order to do more than one kind of calculation. ENIAC, for example, not only calculated artillery firing tables for the US Army, it worked in secret to study the feasibility of thermonuclear weapons.

- This was a huge breakthrough. Programmable computers guided astronauts from Earth to the moon and were reprogrammed during Apollo 13’s troubled mission to bring its astronauts safely back to Earth.

- You’ve grown up during the **Era of Programming**. It even drives the phone you hold in your hand. But the dark data problem has also grown. Modern businesses and technology generate so much data that even the finest programmable supercomputer can't analyze it before the “heat-death” of the universe. Electronic computing is facing a crisis.

### The Era of AI

The history of artificial intelligence dates back to philosophers thinking about the question, "What more can be done with the world we live in?" This question lead to discussions and the very beginning of many ideas about the possibilities involving technology. 

Since the advent of electronic computing, there are some important events and milestones in the evolution of artificial intelligence to know about. Here's an overview to get started.

- **Turing Machine:** Alan Turing publishes Computing Machinery and Intelligence. In the paper, Turing—famous for helping to break the Nazis’ Enigma code during World War II—proposes to answer the question "can machines think?" and introduces the Turing Test to determine if a computer can demonstrate the same intelligence (or the results of the same intelligence) as a human.
- **Dartmouth Conference - Birth of AI:** John McCarthy coins the term "artificial intelligence at the first-ever AI conference at Dartmouth College. Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the **Logic Theorist**, the first-ever running AI software program. McCarthy would go on to invent the Lisp language.
- **1st AI Winter:** 1st AI Winter caused by high expectations from end users and reduced funding.
- **2nd AI Winter:** 2nd AI Winter caused by unmet expectations and computing power.
- **Deep Blue:** IBM Deep Blue beats then world champion Garry Kasparov in a chess match (and rematch).
- **IBM Watson** beats champions Ken Jennings and Brad Rutter at the US game show called Jeopardy!
- **DeepMind's AlphaGo** program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Google bought DeepMind for a reported USD 400 million in 2014.
- IBM unveils **Project Debater**, the first AI system capable of engaging with humans on complex topics in a live debate.

![image](https://github.com/user-attachments/assets/bafc3bbe-9327-48aa-a85d-ce8a84777af3)

#### The Era of AI began one summer in 1956

Early in the summer of 1956, a small group of researchers, led by John McCarthy and Marvin Minsky, gathered at Dartmouth College in New Hampshire. There, at one of the oldest colleges in the United States, they launched a revolution in scientific research and coined the term “artificial intelligence”.

The researchers proposed that “every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.” They called their vision “artificial intelligence” and they raised millions of dollars to achieve it within 20 years. During the next two decades, they accomplished tremendous things, creating machines that could prove geometry theorems, speak simple English, and even solve word problems with algebra. 

> For a short time, AI was one of the most exciting fields

#### But then came winter

By the early 1970s, it became clear that the problem was larger than researchers imagined. There were fundamental limits that no amount of money and effort could solve.

- **Limited Calculating Power:** Today, it is important for a computer to have enough processing power and memory. Every ad you see for companies like Apple or Dell emphasizes how fast their processors run and how much data they can work with. But in 1976, scientists realized that even the most successful computers of the day, working with natural language, could only manipulate a vocabulary of about 20 words. But a task like matching the performance of the human retina might require millions of instructions per second, at a time when the world’s fastest computer could run only about a hundred. By the early 1970s, it became clear that the problem was larger than researchers imagined. There were fundamental limits that no amount of money and effort could solve.

- **Limited Information Storage:** Even simple, commonsense reasoning requires a lot of information to back it up. But no one in 1970 knew how to build a database large enough to hold even the information known by a 2-year-old child.

As these issues became clear, the money dried up for The First Winter of AI.

#### The weather was rough for half a century

It took about a decade for technology and AI theory to catch up, primarily with **new forms of AI called “expert systems”**. These were limited to specific knowledge that could be manipulated with sets of rules. They worked well enough—for a while—and became popular in the 1980s. Money poured in. Researchers invested in tremendous mainframe machines that cost millions of dollars and occupied entire floors of large university and corporate buildings. It seemed as if AI was booming once again.

But soon the needs of scientists, businesses, and governments outgrew even these new systems. Again, funding for AI collapsed.

#### Then came another AI chill

In the late 1980s, the boom in AI research cooled, in part, because of the **rise of personal computers**. Machines from Apple and IBM, sitting on desks in people’s homes, grew more powerful than the huge corporate systems purchased just a few years earlier. Businesses and governments stopped investing in large-scale computing research, and funding dried up.

Over 300 AI companies shut down or went bankrupt during The Second Winter of AI.

#### Now, the forecast is sunny

In the mid-1990s, almost half a century after the Dartmouth research project, the Second Winter of AI began to thaw. Behind the scenes, **computer processing finally reached speeds fast enough for machines to solve complex problems**.

At the same time, the public began to see AI’s ability to play sophisticated games.

- In 1997, IBM’s Deep Blue beat the world’s chess champion by processing over 200 million possible moves per second.
- In 2005, a Stanford University robot drove itself down a 131-mile desert trail.
- In 2011, IBM’s Watson defeated two grand champions in the game of Jeopardy!

Today, AI has proven its ability in fields ranging from cancer research and big data analysis to defense systems and energy production. Artificial intelligence has come of age. AI has become one of the hottest fields of computer science. Its achievements impact people every day and its abilities increase exponentially. The Two Winters of AI have ended!

## Structured, semi-structured, or unstructured data: What are the differences?

### A look at the types of data

Data is raw information. Data might be facts, statistics, opinions, or any kind of content that is recorded in some format. This could include voices, photos, names, and even dance moves!

Data can be organized into the following three types.

Data is raw information. Data might be facts, statistics, opinions, or any kind of content that is recorded in some format. This could include voices, photos, names, and even dance moves!

Data can be organized into the following three types.

- **Structured data** is typically categorized as quantitative data and is highly organized. Structured data is information that can be organized in rows and columns. Perhaps you've seen structured data in a spreadsheet, like Google Sheets or Microsoft Excel. Examples of structured data includes names, dates, addresses, credit card numbers, stock information.
- **Unstructured data**, also known as dark data, is typically categorized as qualitative data. It cannot be processed and analyzed by conventional data tools and methods. Unstructured data lacks any built-in organization, or structure. Examples of unstructured data include images, texts, customer comments, medical records, and even song lyrics.
- **Semi-structured data** is the “bridge” between structured and unstructured data. It doesn't have a predefined data model. It combines features of both structured data and unstructured data. It's more complex than structured data, yet easier to store than unstructured data. Semi-structured data uses metadata to identify specific data characteristics and scale data into records and preset fields. Metadata ultimately enables semi-structured data to be better cataloged, searched, and analyzed than unstructured data. An example of semi-structured data is a video on a social media site. The video by itself is unstructured data, but a video typically has text for the internet to easily categorize that information, such as through a hashtag to identify a location.

> The importance of unstructured data is rapidly increasing. Recent projections(opens in a new tab) indicate that 95% of businesses prioritize unstructured data management.

Are you wondering why this is important? Why would anyone want to search through a mountain of data such as social media posts? 

Here are just some of the many people and organizations that might need to do this:

- A sneaker designer looking for new trends
- Governments searching for possible terrorists
- Pandemic experts trying to anticipate disease outbreaks
- Financial institutions preparing for good times or a recession

> Experts estimate that about 80% of all the data in today’s world is unstructured. It contains so many variables and changes so quickly that no conventional computer program can learn much from it.

### Analyzing unstructured data

Now, imagine a programmable computer trying to extract meaning from billions of data like this! What kind of program would someone write that could sort out every eventuality among the clutter? How would someone build a long enough list of keywords to find anything useful? Unstructured data hides answers to disease prevention, criminal activity, stock markets—almost every aspect of civilization today. Without those answers, people and organizations cannot make useful predictions or recommendations.

But AI can shed light on unstructured data! AI uses new kinds of computing—some modeled on the human brain—to rapidly give dark data structure, and from it, make new discoveries. AI can even learn things—by itself—from the data it manages and teach itself how to make better predictions over time. This is the Era of AI, and it changes everything!
