# Day - 1

## Introduction to Artificial Intelligence

### What is AI?
Artificial intelligence (AI) refers to the ability of a machine to learn patterns and make predictions. AI does not replace human decisions; instead, AI adds value to human judgment. 

In its simplest form, artificial intelligence is a field that combines computer science and robust datasets to enable problem-solving.

### What is the difference between AI and augmented intelligence?
> When learning about artificial intelligence, you’ll come across the term augmented intelligence. Both terms share the same objective, but have different approaches. 
- Augmented intelligence has a modest goal of helping humans with tasks that are not practical to do. For example, “reading” 1000 pages in an hour.
- In contrast, artificial intelligence has a lofty goal of mimicking human thinking and processes.
However, it’s important to note that AI today is not mature enough to perform independent tasks such as diagnosing cancer.

#### Real-Life Analogy: My Commute

Driving to work involves three forms of intelligence:
- Human intelligence – Used for basic driving tasks like steering and checking mirrors.
- Artificial Intelligence: Took over on the highway via the car’s self-driving mode—managing speed, lane control, and safe distance with no human input.
- Augmented Intelligence: Kicked in with features like collision detection and blind spot warnings, which assisted the driver without replacing them.
It made me realize: AI replaces, but Augmented AI empowers.

#### Key Differences:

* **AI**: Replaces human decision-making to perform tasks such as reasoning, problem-solving, and communication.
* **Augmented Intelligence**: Enhances human abilities through technology—like screen readers or driver-assist tools.

#### Strengths Matrix:

| Strengths           | Humans    | Machines (AI)          |
| ------------------- | --------- | ---------------------- |
| Data Processing     | Limited   | Fast and high-volume   |
| Creativity          | Strong    | Weak                   |
| Emotional Skills    | Excellent | Lacking                |
| Accuracy in Repeats | Can vary  | Consistent and precise |

**Conclusion**: The best results often come from **both AI and humans working together**—a balance found in **Augmented Intelligence**.

### What continues to drive the development of AI? 

As computing power and algorithms become more powerful and data volumes increase, companies will adopt new use cases for AI technologies. Companies will embed smart systems into their applications to drive innovation and efficiencies, enhance employee experience, automate tasks, decrease costs, and improve revenue.

### What does AI do?

> Artificial intelligence machines (researchers call them “AI services”) don’t think. They calculate. <br>

They represent some of the newest, most sophisticated calculating machines in human history. Some can perform what’s called machine learning as they acquire new data. Others, using calculations arranged in ways inspired by neurons in the human brain, can even perform deep learning with multiple levels of calculations.

#### Task

> Imagine you are given the job to sort items in the produce department at a grocery store. You realize that there are dozens of products and very little time to sort them manually. How could you use artificial intelligence, machine learning, and deep learning to help with your work?

#### Artificial Intelligence:

To separate the stone fruits, berries, and tropical fruits, you could create **a programmed rule in the format of if-else statements**. This allows the machine to recognize what is on the label and route it to the correct basket. 

A programmed rule might look something like this:

```
if berries_is_on_label:
   route_items_to_center_basket()

else:
   redirect_item_to_main_basket()
```
 
Artificial intelligence makes this process more efficient.<br><br>
![image](https://github.com/user-attachments/assets/51b8ccdf-4d3f-4828-8b57-97707acbf912)

#### Machine Learning:

To improve the performance of the machine, you expose it to more data to ensure that the **machine is trained on numerous characteristics of each type of fruit**. The more data you provide for the algorithm, the better the model gets. By providing more data and adjusting parameters, the machine minimizes errors by repetitive guess work.


![image](https://github.com/user-attachments/assets/7f19b92c-793e-4067-a9e9-10e1bf91bcd9)

#### Deep Learning:

The grocery store has expanded its produce selection to include more varieties such as nectarines and plums (stone fruits), blackberries and cranberries (berries), and mangoes and star fruit (tropical fruits). In addition, the products now come in different sizes, shapes, and colors. What makes deep learning different?

Deep learning models **eliminate the need for feature extractions**. For your work in the product department, you decide to use algorithms based on deep learning to sort fruit by removing the need to define what each product looks like. Feature extraction is built into the process without human input. Once you have provided the deep learning model with dozens of fruit pictures, it processes the images through different layers of neural networks. The layers can then learn an implicit representation of the raw data on their own.

![image](https://github.com/user-attachments/assets/6e5ef1f6-9452-4bc9-8d3a-7e88c40088ae)


### How do AI services calculate?

- **Analyze**: AI services can take in (or “ingest”) enormous amounts of data. They can apply mathematical calculations to analyze data, sorting and organizing it in ways that would have been considered impossible only a few years ago.
- **Prediction**: AI services can use their data analysis to make predictions. They can, in effect, say, “Based on this information, a certain thing will probably happen.”

### What predictions can AI make?

Most people have a love-hate relationship with the autocorrect feature on phones or computers. What’s happening when you enter a misspelled word? And how does the machine know to suggest a better spelling?

Simply put, the software analyzes what you’ve typed so far and predicts a likely correction. Your phone or computer (or its online service) has more than just a dictionary of correct spellings. It has a huge library of phrases that humans use in certain contexts on many subjects. So, when you enter a word that’s not in its dictionary, it begins analyzing and predicting and suggests the word you need. Predictions aren’t always accurate. But if they’re correct often enough, they’re useful and can save you time.

- **Human Language**: Online chatbots use natural language processing (NLP) to analyze poorly typed or spoken questions, then predict which answers to give on topics ranging from shipping or business hours to merchandise and sizes.
- **Vision Learning**: AI helps doctors identify serious diseases based on unusual symptoms and early-warning signs, and it reads speed limit and stop signs as it guides cars through traffic.
- **Fraud Detection**: AI analyzes patterns created when thousands of bank customers make credit card purchases, then predicts which charges might be the result of identity theft.

### How is AI evolving?

Computer scientists have identified three levels of AI based on predicted growth in its ability to analyze data and make predictions. They call these levels:
- Narrow AI
- Broad AI
- General AI

![image](https://github.com/user-attachments/assets/4c3a5922-d301-4804-93ca-43fc607de2f4)

#### Narrow AI:

- Narrow AI is focused on addressing a single task such as predicting your next purchase or planning your day. 
- Narrow AI is scaling very quickly in the consumer world, in which there are a lot of common tasks and data to train AI systems. For example, you can buy a book with a voice-based device. 
- Narrow AI also enables robust applications, such as using Siri on an iPhone, the Amazon recommendation engine, autonomous vehicles, and more. Narrow AI systems like Siri have conversational capabilities, but only if you stick to the script.

#### Broad AI:

- Broad AI is a midpoint between Narrow and General AI. 
- Rather than being limited to a single task, Broad AI systems are more versatile and can handle a wider range of related tasks. 
- Broad AI is focused on integrating AI within a specific business process where companies need business- and enterprise-specific knowledge and data to train this type of system. 
- Newer Broad AI systems predict global weather, trace pandemics, and help businesses predict future trends.

#### General AI:

- General AI refers to machines that can perform any intellectual task that a human can. 
- Currently, AI does not have the ability to think abstractly, strategize, and use previous experiences to come up with new, creative ideas as humans do, such as inventing a new product or responding to people with appropriate emotions. And don't worry, AI is nowhere near this point.

There might be another level, known as artificial superintelligence (ASI) that could appear near the end of this century. Then machines might become self-aware! Even then, no levels of AI are expected to replace or dominate you. Instead, scientists hope AI will extend humans’ ability to lead richer lives.


# Day - 2

## What are the three eras of computing?

### The Era of Tabulation

For centuries, people have struggled to understand the meaning that’s hidden in large amounts of data. After all, it’s one thing to estimate how many trees grow in a million square miles of forest. It’s something else to classify what species of trees they are, how they cluster at different altitudes, and what could be built with the wood they provide. That information can be difficult to extract from a very large amount of data. 

> Because it's hard to see without help, scientists call this **dark data**. It’s information without a structure: just a huge, unsorted mess of facts.

- To sort out unstructured data, humans have created many different calculating machines. Over 2000 years ago, tax collectors for Emperor Qin Shihuang used the abacus—a device with beads on wires—to break down tax receipts and arrange them into categories. From this, they could determine how much the Emperor should spend on building extensions to the Great Wall of China.
- In England during the mid-1800s, Charles Babbage and Ada Lovelace designed (but never finished) what they called a “difference engine” designed to handle complex calculations using logarithms and trigonometry. Had they built it, the difference engine might have helped the English Navy build tables of ocean tides and depth soundings that could guide English sailors through rough waters.
- By the late 1880s, people were thinking about how to develop faster systems to record data. Herman Hollerith, inspired by train conductors using holes punched in different positions on a railway ticket to record traveler details, invented the recording of data on a machine-readable punched card. Hollerith’s cards were used for the 1890 US Census, which finished months ahead of schedule and under budget. Later versions of tabulating machines had broad applications in business, such as financial accounting and data processing.

The word to remember across those twenty centuries is tabulate. Think of **tabulation as “slicing and dicing”** data to give it a structure, so that people can uncover patterns of useful information. You tabulate when you want to get a feel for what all those columns and rows of data in a table really mean.

Researchers call these centuries the **Era of Tabulation**, a time when machines helped humans sort data into structures to reveal its secrets.


### The Era of Programming

- During the turmoil of World War II, a new approach to dark data emerged: the Era of Programming. Scientists began building electronic computers, like the Electronic Numerical Integrator and Computer (ENIAC) at the University of Pennsylvania, that could run more than one kind of instruction (today we call those “programs”) in order to do more than one kind of calculation. ENIAC, for example, not only calculated artillery firing tables for the US Army, it worked in secret to study the feasibility of thermonuclear weapons.

- This was a huge breakthrough. Programmable computers guided astronauts from Earth to the moon and were reprogrammed during Apollo 13’s troubled mission to bring its astronauts safely back to Earth.

- You’ve grown up during the **Era of Programming**. It even drives the phone you hold in your hand. But the dark data problem has also grown. Modern businesses and technology generate so much data that even the finest programmable supercomputer can't analyze it before the “heat-death” of the universe. Electronic computing is facing a crisis.

### The Era of AI

The history of artificial intelligence dates back to philosophers thinking about the question, "What more can be done with the world we live in?" This question lead to discussions and the very beginning of many ideas about the possibilities involving technology. 

Since the advent of electronic computing, there are some important events and milestones in the evolution of artificial intelligence to know about. Here's an overview to get started.

- **Turing Machine:** Alan Turing publishes Computing Machinery and Intelligence. In the paper, Turing—famous for helping to break the Nazis’ Enigma code during World War II—proposes to answer the question "can machines think?" and introduces the Turing Test to determine if a computer can demonstrate the same intelligence (or the results of the same intelligence) as a human.
- **Dartmouth Conference - Birth of AI:** John McCarthy coins the term "artificial intelligence at the first-ever AI conference at Dartmouth College. Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the **Logic Theorist**, the first-ever running AI software program. McCarthy would go on to invent the Lisp language.
- **1st AI Winter:** 1st AI Winter caused by high expectations from end users and reduced funding.
- **2nd AI Winter:** 2nd AI Winter caused by unmet expectations and computing power.
- **Deep Blue:** IBM Deep Blue beats then world champion Garry Kasparov in a chess match (and rematch).
- **IBM Watson** beats champions Ken Jennings and Brad Rutter at the US game show called Jeopardy!
- **DeepMind's AlphaGo** program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Google bought DeepMind for a reported USD 400 million in 2014.
- IBM unveils **Project Debater**, the first AI system capable of engaging with humans on complex topics in a live debate.

![image](https://github.com/user-attachments/assets/bafc3bbe-9327-48aa-a85d-ce8a84777af3)

#### The Era of AI began one summer in 1956

Early in the summer of 1956, a small group of researchers, led by John McCarthy and Marvin Minsky, gathered at Dartmouth College in New Hampshire. There, at one of the oldest colleges in the United States, they launched a revolution in scientific research and coined the term “artificial intelligence”.

The researchers proposed that “every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.” They called their vision “artificial intelligence” and they raised millions of dollars to achieve it within 20 years. During the next two decades, they accomplished tremendous things, creating machines that could prove geometry theorems, speak simple English, and even solve word problems with algebra. 

> For a short time, AI was one of the most exciting fields

#### But then came winter

By the early 1970s, it became clear that the problem was larger than researchers imagined. There were fundamental limits that no amount of money and effort could solve.

- **Limited Calculating Power:** Today, it is important for a computer to have enough processing power and memory. Every ad you see for companies like Apple or Dell emphasizes how fast their processors run and how much data they can work with. But in 1976, scientists realized that even the most successful computers of the day, working with natural language, could only manipulate a vocabulary of about 20 words. But a task like matching the performance of the human retina might require millions of instructions per second, at a time when the world’s fastest computer could run only about a hundred. By the early 1970s, it became clear that the problem was larger than researchers imagined. There were fundamental limits that no amount of money and effort could solve.

- **Limited Information Storage:** Even simple, commonsense reasoning requires a lot of information to back it up. But no one in 1970 knew how to build a database large enough to hold even the information known by a 2-year-old child.

As these issues became clear, the money dried up for The First Winter of AI.

#### The weather was rough for half a century

It took about a decade for technology and AI theory to catch up, primarily with **new forms of AI called “expert systems”**. These were limited to specific knowledge that could be manipulated with sets of rules. They worked well enough—for a while—and became popular in the 1980s. Money poured in. Researchers invested in tremendous mainframe machines that cost millions of dollars and occupied entire floors of large university and corporate buildings. It seemed as if AI was booming once again.

But soon the needs of scientists, businesses, and governments outgrew even these new systems. Again, funding for AI collapsed.

#### Then came another AI chill

In the late 1980s, the boom in AI research cooled, in part, because of the **rise of personal computers**. Machines from Apple and IBM, sitting on desks in people’s homes, grew more powerful than the huge corporate systems purchased just a few years earlier. Businesses and governments stopped investing in large-scale computing research, and funding dried up.

Over 300 AI companies shut down or went bankrupt during The Second Winter of AI.

#### Now, the forecast is sunny

In the mid-1990s, almost half a century after the Dartmouth research project, the Second Winter of AI began to thaw. Behind the scenes, **computer processing finally reached speeds fast enough for machines to solve complex problems**.

At the same time, the public began to see AI’s ability to play sophisticated games.

- In 1997, IBM’s Deep Blue beat the world’s chess champion by processing over 200 million possible moves per second.
- In 2005, a Stanford University robot drove itself down a 131-mile desert trail.
- In 2011, IBM’s Watson defeated two grand champions in the game of Jeopardy!

Today, AI has proven its ability in fields ranging from cancer research and big data analysis to defense systems and energy production. Artificial intelligence has come of age. AI has become one of the hottest fields of computer science. Its achievements impact people every day and its abilities increase exponentially. The Two Winters of AI have ended!

## Structured, semi-structured, or unstructured data: What are the differences?

### A look at the types of data

Data is raw information. Data might be facts, statistics, opinions, or any kind of content that is recorded in some format. This could include voices, photos, names, and even dance moves!

Data can be organized into the following three types.

Data is raw information. Data might be facts, statistics, opinions, or any kind of content that is recorded in some format. This could include voices, photos, names, and even dance moves!

Data can be organized into the following three types.

- **Structured data** is typically categorized as quantitative data and is highly organized. Structured data is information that can be organized in rows and columns. Perhaps you've seen structured data in a spreadsheet, like Google Sheets or Microsoft Excel. Examples of structured data includes names, dates, addresses, credit card numbers, stock information.
- **Unstructured data**, also known as dark data, is typically categorized as qualitative data. It cannot be processed and analyzed by conventional data tools and methods. Unstructured data lacks any built-in organization, or structure. Examples of unstructured data include images, texts, customer comments, medical records, and even song lyrics.
- **Semi-structured data** is the “bridge” between structured and unstructured data. It doesn't have a predefined data model. It combines features of both structured data and unstructured data. It's more complex than structured data, yet easier to store than unstructured data. Semi-structured data uses metadata to identify specific data characteristics and scale data into records and preset fields. Metadata ultimately enables semi-structured data to be better cataloged, searched, and analyzed than unstructured data. An example of semi-structured data is a video on a social media site. The video by itself is unstructured data, but a video typically has text for the internet to easily categorize that information, such as through a hashtag to identify a location.

> The importance of unstructured data is rapidly increasing. Recent projections(opens in a new tab) indicate that 95% of businesses prioritize unstructured data management.

Are you wondering why this is important? Why would anyone want to search through a mountain of data such as social media posts? 

Here are just some of the many people and organizations that might need to do this:

- A sneaker designer looking for new trends
- Governments searching for possible terrorists
- Pandemic experts trying to anticipate disease outbreaks
- Financial institutions preparing for good times or a recession

> Experts estimate that about 80% of all the data in today’s world is unstructured. It contains so many variables and changes so quickly that no conventional computer program can learn much from it.

### Analyzing unstructured data

Now, imagine a programmable computer trying to extract meaning from billions of data like this! What kind of program would someone write that could sort out every eventuality among the clutter? How would someone build a long enough list of keywords to find anything useful? Unstructured data hides answers to disease prevention, criminal activity, stock markets—almost every aspect of civilization today. Without those answers, people and organizations cannot make useful predictions or recommendations.

But AI can shed light on unstructured data! AI uses new kinds of computing—some modeled on the human brain—to rapidly give dark data structure, and from it, make new discoveries. AI can even learn things—by itself—from the data it manages and teach itself how to make better predictions over time. This is the Era of AI, and it changes everything!

# Day - 3

## Is machine learning the answer to the unstructured data problem?

### How does machine learning approach a problem?

If AI doesn’t rely on programming instructions to work with unstructured data, how does AI do it? Machine learning can analyze dark data far more quickly than a programmable computer can. To see why, consider the problem of finding a route through big city traffic using a navigation system. It’s a dark data problem because solving it requires working with not only a complicated street map, but also with changing variables like weather, traffic jams, and accidents. Let’s look at how two different systems might try to solve this problem.

- **Programmable Computer:** <br>
Researchers might upload onto the computer a complete database of all possible routes through the city. This is an enormous collection of structured data.

Then they would have to add much more data describing current weather and traffic conditions. This would have to be revised every few minutes for the entire city!

Then they might use a programmable computer to search through the data until it finds a route from start to finish. The entire project would require astronomical resources and time, if it could be accomplished at all!

- **AI with Machine Learning:** <br>

The machine learning AI would treat the problem like climbing a tree. The system would try a route, as if starting at the base of the trunk. Upon reaching a branch, the system would then fork in one direction and continue doing so until it reached either a dead end or the desired destination.

It would do this over and over again, then compare successful routes to identify the shortest one. Although the work sounds repetitious, it requires fewer resources and can be completed more quickly.

### The machine learning process is entirely different

The machine learning process has advantages:

- It doesn’t need a database of all the possible routes from one place to another. It just needs to know where places are on the map.
- It can respond to traffic problems quickly because it doesn’t need to store alternative routes for every possible traffic situation. It notes where slowdowns are and finds a way around them through trial and error.
- It can work very quickly. While trying single turns one at a time, it can work through millions of tiny calculations.

But machine learning has two more advantages that programmable computers lack:

- **Machine learning can predict**. You know this already. A machine can determine, “Based on traffic right now, this route is likely to be faster than that one.” It knows this because it compared routes as it built them.
- **Machine learning learns!** It can notice that your car was delayed by a temporary detour and adjust its recommendations to help other drivers.

### Machine learning uses probabilistic calculation

There are two other ways to contrast classical and machine learning systems. One is deterministic and the other is probabilistic.

Let’s dig in and see what these two words mean.

- For a **deterministic** system, there must be an enormous, predetermined structure of routes—a gigantic database of possibilities from which the machine can make its choice. If a certain route leads to the destination, then the machine flags it as “YES”. If not, it flags it as “NO”. This is basically binary thinking: on or off, yes or no. This is the essence of a computer program. The answer is either true or false, not a confidence value.

- Machine learning is **probabilistic**. It never says “YES” or “NO”. Machine learning is analog (like waves gradually going up and down) rather than binary (like arrows pointing upward and downward). Machine learning constructs every possible route to a destination and compares them in real time, including all the variables such as changing traffic. So, a machine learning system doesn’t say, “This is the fastest route.” It says something like, “I am 84% confident that this route will get you there in the shortest time.” You might have seen this yourself if you’ve traveled in a car with an up-to-date GPS navigation system that offers you two or three choices with estimated times.

### Machine learning enables a rich partnership between technology and humans

AI systems and humans excel at different things. For example, you, as a person, might excel at imagining possibilities, while AI excels at pinpointing patterns.

### Does common sense make sense?

It turns out that in fields, ranging from medicine and education to social studies and government, the best decisions are made using a **balance of human and machine strengths**. But remember, there is another elusive but vital capability that must also be considered: common sense. You might know people with strong common sense and understand its value. You also might have seen or read output from machines that makes no sense. Yet, there’s a contribution to be made from both sides.

Common sense draws on many **complex generalizations mixed with compassion and abstractions**. At this time, only humans can use common sense well. The problem is that common sense is often tainted with bias that can distort your judgment. AI systems can balance this. As long as AI systems are provided and trained with unbiased data, they can make recommendations that are free of bias. A partnership between humans and machines can lead to sensible decisions. 


## Three common methods of machine learning

Machine learning solves problems in three ways: 

- Supervised learning
- Unsupervised learning
- Reinforcement learning


### Supervised learning

**Supervised learning** is about providing AI with enough examples to make accurate predictions. 

All supervised learning algorithms need **labeled data**. Labeled data is data that is grouped into samples that are tagged with one or more labels. In other words, applying supervised learning requires you to tell your model:

- What the key characteristics of a thing are, also called **features**
- What the thing actually is

For example, the information might be drawings and photos of animals, some of which are dogs and are labeled “dog”. The machine will learn by identifying a pattern for “dog”. When the machine sees a new dog photo and is asked, “What is this?”, it will respond, “dog”, with high accuracy. This is known as a classification problem.

### Unsupervised learning

In **unsupervised learning**, a person feeds a machine a large amount of information, asks a question, and then the machine is left to figure out how to answer the question by itself.

For example, the machine might be fed many photos and articles about dogs. It will classify and cluster information about all of them. When shown a new photo of a dog, the machine can identify the photo as a dog, with reasonable accuracy.

Unsupervised learning is helpful when you don't know how to classify data. For example, imagine you work for a banking institution and you have a large set of customer financial data. You don't know what type of groups or categories to organize the data. Here, an unsupervised learning algorithm could find natural groupings of similar customers in a database, and then you could describe and label them. 

This type of learning has the **ability to discover similarities and differences in information**, which makes it an ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition.

### Reinforcement learning

Reinforcement learning is a machine learning model similar to supervised learning, but the algorithm isn’t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes is reinforced to develop the best recommendation for a given problem. The foundation of reinforcement learning is rewarding the “right” behavior and punishing the “wrong” behavior.

You might be wondering, what does it mean to "reward" a machine? Good question! Rewarding a machine means that you give your agent positive reinforcement for performing the "right" thing and negative reinforcement for performing the "wrong" things. 

As a machine learns through trial and error, it tries a prediction, then compares it with data in its corpus. 

- Each time the comparison is positive, the machine receives positive numerical feedback, or a **reward**.
- Each time the comparison is negative, the machine receives negative numerical feedback, or a **penalty**.
Over time, a machine’s predictions will grow to be more accurate. It accomplishes this automatically based on feedback, rather than through human intervention.

## How will machine learning transform human life?

Perhaps, 25 years from now, General AI is expected to emerge. AI researcher Nick Bostrom defines this superintelligence as, “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.” You’re likely to see General AI appear in your lifetime. General AI will enable supersmart bots and technologies to link AI with the Internet of Things through “embodied cognition”. This will give machines the ability to interact in human-like ways as they work alongside humans.

- **AI Everywhere:** AI will move into all industries, from finance, to education, to healthcare. AI will increase productivity and enable new opportunities.
- **Deeper Insights:** New technologies will sense, analyze, and understand things never before possible.
- **Engagement Reimagined:** New forms of human-machine interaction and emerging technologies, such as conversational bots, will transform how humans engage with each other and with machines.
- **Personalization:** Machine interactions will be personalized for you, with new levels of detail and scale.
- **Instrumented Planet:** Billions of sensors generating exabytes of data will open new possibilities for improving Earth’s safety, sustainability, and security.

What’s beyond these wonders? Humans, devices, and robots might exist as a collective “digital brain” that anticipates human needs, makes predictions, and provides solutions. Farther in the future, we might trust the digital brain to do things on our behalf across a broad spectrum of endeavors!

# Day - 4

## How do machines learn?

Supervised learning requires that an AI system ingest structured data. Unsupervised learning and reinforcement learning require a system to develop its own structure either by analyzing large amounts of data (unsupervised learning) or by trial-and-error (reinforcement learning).

## Classical machine learning

Classical machine learning began in the 1950s. AI systems learned by ingesting data and getting better at recognizing patterns. The AI systems could predict things like the distance between points or the intensity of values.

Like all machine learning, the classical form depends on algorithms. Recall that algorithms are mathematical expressions that output a result. C**lassical machine learning uses a small number of algorithms in a relatively simple arrangement**. Sometimes machine learning algorithms are **binary**, which means that they output one of only two values. Typical binary results might be a 1 or a 0, a YES or a NO, and a TRUE or a FALSE.

Other classical learning algorithms are more complicated. For example, their result might be represented as a position on a multidimensional graph rather than “this point” or “that point”. Here are three typical algorithms that are used in classical computing:

- Decision tree
- Linear regression
- Logistic regression

## Decision tree
A decision tree is a supervised learning algorithm. It operates like a flowchart. You can think of a flowchart as an upside-down decision tree. The flowchart has a root node (where the flowchart begins), branches that connect to internal nodes, and more branches that connect to leaf nodes.

![image](https://github.com/user-attachments/assets/0de07ede-2a9a-4e01-8ccd-311f48231afd)

## Linear regression
Linear regression is another type of algorithm. It relates to data that might be graphed as a straight line. For example, a business might believe that more advertising spending leads to better sales. This could be graphed as a series of dots that form a rising straight line, as depicted here.

![image](https://github.com/user-attachments/assets/dc54c1b1-d3cc-4c47-874e-de64e73ca376)

As suggested in the chart, as advertising increases, so do sales. There are many possible outcomes (different amounts of advertising lead to different amounts of sales), but the **change rises on the graph in a straight line**.

The situation is more complicated if a company’s actual sales show different data for different products, at different locations, on different dates, and so on. With a large number of variables and instances, the graph becomes a mass of dots that don’t arrange into a straight line at all. Without adjustment, resulting graph is too general to help a business make a good decision. That’s where linear regression can help. Linear regression can learn all the variables, then calculate a reasonably accurate prediction of how advertising will impact sales at some time and location in the future. In effect, **linear regression resolves the mass of dots into a “most likely”** line that can be used for simple prediction.

## Logistic regression
In some situations, a relationship does not fall in a straight line. Sometimes a system uses values that require a specific, limited kind of outcome, such as something between 0 and 1 (or NO and YES). In this situation, a graph can form what’s called a sigmoid function, or an S-shaped curve, as shown in the accompanying example. For any set of variables, the outcome (which is a point on the S-curve) falls between 0 and 1.

![image](https://github.com/user-attachments/assets/c52d2512-fc05-40e3-8a16-e7c6bcf5dbcd)

Here’s a real-world example. Refer to the previous graph. Let’s say you want to know how many hours you should study in order to pass an exam. You have the number of study hours and passing or failing status for 10 other students. “Hours of studying” is a varying amount, in this case, between 1 and 5 hours. Passing the exam is a matter of NO or YES (either FAIL or PASS).

If you plot these two factors together as a logistical regression, you get an S curve in which 0 hours of study results in a very low chance of passing, while 5.5 hours results in a very high chance. As shown in the chart, the variable “Hours of studying” is along the x-axis. The values along the y-axis represent the values for the variable “Probability of passing exam”.

Here’s another way to understand the graph: it predicts that studying at least 4 hours gives you a very good chance of passing the course.

### Comparing linear and logistic regressions

Linear and logistic regressions are useful in the following ways:

- A **linear** regression answers a question such as “If this increases by X, how much will Y increase?”
- A **logistic** regression answers a question such as “If this increases by X, will the value of Y be closer to 0 or 1?”

## Classical machine learning is not obsolete
Classical machine learning can be outperformed, at some tasks, by newer methods that are part of the deep learning ecosystem. But there are still reasons to use classical machine learning. These include:

- **Work with structured data:**  Classical machine learning is used mostly with structured data from databases, such as hours studied compared to grades earned.
- **Lower expense to operate:** Classical machine learning requires less computing power than deep learning ecosystems. They can run on less expensive computers with less powerful processors, which lowers the price for smaller businesses, communities, or healthcare systems that share time on them in pay-as-you-go arrangements.
- **Easier to interpret:** Deep networks are so complex that even AI researchers don’t entirely understand what’s going on inside. As a result, AI researchers are not always able to determine when deep network systems are producing invalid outputs. Compared to these mysteries, classical results can be easier to debug, and to test for accuracy and lack of bias.

# Day - 5

## The deep learning ecosystem

Today, machine learning has evolved into a collection of powerful applications called the deep learning ecosystem. The foundation for many applications is called a neural network. A neural network uses electronic circuitry inspired by the way neurons communicate in the human brain. 

In the brain, cells called neurons have a cell body at one end where the nucleus resides, and a long axon leading to a set of branching terminals at the other end. Neurons communicate to each other by receiving signals into the axon, altering those signals, then transmitting them out through the terminals to other neurons. Researchers estimate that a human brain has about 100 billion neurons, each one connected to up to 10,000 other neurons.

In a neural network, a building block, called a perceptron, acts as the equivalent of a single neuron. A perceptron has an input layer, one or more hidden layers, and an output layer. A signal enters the input layer and the hidden layers run algorithms on the signal. Then, the result is passed to the output layer.

The operation of a neural network is pure mathematics. The network isn’t “thinking”; it is calculating. But it’s using those calculations to create an output that humans can interpret as an answer or a recommendation.

## How does a neural network learns in the first place. 

The answer is: by continuously adjusting itself, in a process that humans refer to as **trial and error**.

Once a neural network has ingested or already learned a certain amount of data, it stores the data in its **“body of information”, called its corpus**. In order to learn, the neural network constantly tests new data or the results of its calculation against its corpus. If the network determines that the new data or results don’t match the patterns it has already established, it modifies those patterns for a better fit. Sometimes, to improve a single match, the network tests hundreds or thousands of modifications very rapidly and makes adjustments. Then, the network tests to determine if the match is improving. So, step by step, the machine learns.

## Machine learning makes many guesses

Machine learning uses its tremendous calculation speed to make many guesses that bring it closer and closer to an answer. It randomly makes its first guess, sets that guess as a variable, then tests how accurately the guess fits with both old and new data. Next, it makes an adjustment to the variable and tries again. Using mathematical processes to help it choose right-size adjustments, the system keeps on trying, getting closer and closer to perfection but never quite reaching it. 

For this reason, many AI systems output a confidence value along with an answer or prediction. For example, a system predicting effective treatments for a cancer patient might output two or three suggested approaches, along with a measure of how confident it is that each treatment might work. This reflects how the system reaches those decisions. The system also leaves the final decision to the doctor who knows the patient.

## What Deep Neural Network can do?

- **Photo identification:** Technologists can use a DNN to examine an historic photo of unknown persons or places. The DNN compares what it finds with millions of pictures in its corpus and then outputs full names and possible locations.
- **Housing Construction:** Real estate companies can use a DNN to predict changes in housing prices across an entire city or state. DNNs can help real estate companies predict business trends and determine how to invest in materials and labor.
- **Self-driving cars:** Automobile engineers can use DNNs to model millions of driving situations and help self-driving vehicles navigate safely.
- **Cancer treatment:** Radiologists can use DNNs to identify variations in MRI images that are otherwise invisible to the human eye. Radiologists can receive early warnings of potentially treatable cancers.

## Generative AI

Generative AI is a type of artificial intelligence that creates new, original content that people have never seen before.

- Most AI systems are discriminative AI models, which predict and classify data.
- In contrast, generative AI models are a type of **deep learning AI system** that uses algorithms to generate content based on a submitted prompt, thus the name of generative AI.
For example, a discriminative model could tell a bicycle from a truck and a generative model could generate a new image that looks like a bicycle.

So, generative AI’s distinction from other AI systems is its ability **to produce content that is new and considered creative**, such as images, videos, music, synthetic data, essays, answers to questions, and more.

![image](https://github.com/user-attachments/assets/5c44565c-90b6-429e-a2dd-c932489f0651)

## How does generative AI work?
Think of generative AI as a virtual artist. Just like a human artist, it needs inspiration and tools to create something unique. Instead of using paint and canvas, however, generative AI uses algorithms and data sets.

Here’s the overall generative AI process. 

1. First, a person feeds the AI a **large amount of data**. This could be anything from images and sounds to text and numbers.

2. Then, the AI analyzes this data, looking for patterns and relationships between the different pieces of information. The **neural network is trained** on a dataset of examples of the type of output it is intended to generate, such as images or text. During the training process, the neural network learns to **identify patterns and relationships in the input data** and use them to generate new outputs that are similar to, but not identical to, the examples it was trained on.

3. Next, the **AI uses what it has learned to create something new**. The neural network generates new outputs by inputting a random seed value. The seed value serves as the starting point for the generation process. The neural network processes the seed value and generates a new output that is based on the patterns and relationships it learned during training. For example, if someone gave the AI a set of images of dogs, it might use its knowledge of different dog breeds to create an image of a new dog that doesn't exist in real life.

Generative AI can also complete more complex tasks, like writing stories or composing music. In these cases, the AI analyzes patterns in language or music to create something entirely new.

Of course, not all generative AI is perfect. Just like human artists, sometimes the results can be a bit strange or unexpected. However, as this technology improves, you can expect to see even more impressive creations from generative AI in the future!

## Types of generative AI models

Let’s explore the three primary types of generative AI models: 

- **Variational autoencoder (VAE):** Think of variational autoencoder (VAE) models as a skilled artist who can look at a painting, quickly sketch a simplified version of it, and then recreate a new painting using only that simplified sketch as a reference. The artist is capturing the essential elements of the painting and then using them to create a new work of art.
VAEs use a similar process. The "encoder" network **compresses the input data into a lower-dimensional representation** and the "decoder" network reconstructs the original data from this compressed representation. This allows VAEs to capture the underlying structure and patterns in the data, which can then generate new, similar data.

![image](https://github.com/user-attachments/assets/c735ceb9-1eec-4045-b2d1-34d5e5e1359f)

- **Generative adversarial network (GAN):** Think of a generative adversarial network (GAN) model as a competition between a skilled forger (the generator) and a talented art critic (the discriminator). The forger creates fake paintings, while the critic tries to determine whether each painting is genuine or a forgery. As the forger improves their technique, the critic becomes more discerning, and this cycle continues until the forger can create near-perfect forgeries.

In GANs, the **generator creates new data**, while the **discriminator evaluates the quality of the generated data**. The generator tries to create data that is realistic enough to fool the discriminator, while the discriminator learns to better distinguish between real and generated data. This competition leads to the generator creating increasingly realistic content.

![image](https://github.com/user-attachments/assets/f9a7cdb8-5e41-4d37-b9a0-4d24c92e3d03)

- **Autoregressive:** Imagine an autoregressive model as a skilled storyteller who listens to the beginning of a story and then continues it by **predicting what comes next** based on the words and events that have occurred so far. The storyteller uses their knowledge of language, grammar, and storytelling conventions to create a coherent and engaging continuation of the story.

Autoregressive models generate new content by predicting the next element in a sequence based on the previous elements. They are particularly well-suited for generating text because they can model the conditional probabilities of words and characters in a sentence.

![image](https://github.com/user-attachments/assets/c783e960-de5b-4cee-91c1-acaaaf0a09ee)

## Examples of generative AI applications

- **CHATGPT:** OpenAI launched ChatGPT, an AI chatbot, in November 2022. Able to **interact by using conversational natural language**, this AI tool goes beyond the traditional search engine responses of simply listing related results. Instead, ChatGPT follows instructions given in the prompt and provides a detailed response. For example, with ChatGPT, a person can enter a text prompt of “Write a poem about cats” and the result will be a poem about cats, rather than a listing of websites about cats.
- **IBM WATSON DISCOVERY:** IBM Watson Discovery uses foundational technologies, such as large language models (LLMs), to **obtain insights from what it is that we don’t know**. It’s used widely in genomic research discovering what amino acids might lurk inside the protein not known before and unearth the relationship of various actors or entities for government security related questions.
- **DALL-E AND DALL-E 2:** Developed by OpenAI, these are generative AI models that **use natural language text input to generate digital images**. The first version of DALL-E could only render AI-created images in a cartoonish fashion, but the latest version can produce much more realistic images due to improved image processing algorithms.
- **BARD:** Bard is a generative AI tool that Google launched in an initial, limited capacity in March 2023. Bard is founded on Google’s Bidirectional Encoder Representations from Transformers (BERT) model. It isn’t generative AI; rather, Google developed it for natural language processing (NLP), especially for its capacity to interpret the nuances of a user’s search words. **Bard builds on BERT’s capability of natural language interactions with Bard’s generative AI capability to generate new content.** For example, musicians can use Bard to compose music and lyrics.

# Day - 6

## Industry uses of generative AI

Generative AI is already impacting, and will increasingly impact, a wide range of industries, including sports, entertainment, healthcare, retail, banking, manufacturing, engineering, security, media, agriculture, and the list goes on and is continuing to expand.

Review these real-world examples of generative AI in practice.

- **Sports:** <br>
In sports, generative AI can help improve athletic performance and enhance fan engagement. 

One application of generative AI in sports is the **creation of personalized workout plans**. By analyzing an athlete's biometric data, generative AI can create customized workout plans tailored to the athlete's fitness level and goals. This technology can **improve athletic performance** by providing athletes with targeted and efficient training regimes. 

Another application of generative AI in sports, **around fan engagement, is the creation of realistic 3D models of athletes for use in sports video games and virtual reality experiences.** By using generative AI, game developers can create highly realistic and personalized virtual athletes that can interact with users in real-time. This technology can enhance fan engagement by providing users with immersive and engaging sports experiences. 

Generative AI can **analyze sports data and identify patterns and trends that can inform coaching strategies and player selection.** 

Overall, generative AI has the potential to transform the way people play, coach, and experience sports, enabling new levels of personalization, efficiency, and engagement.



**Use case:** <br>
The Miami Dolphins, an American football team in the United States, are using generative AI to improve player performance and prevent injury. They are collaborating with a company called Blue River Technology **to develop a computer vision system that uses generative AI to analyze video footage of players and identify areas for improvement.** The system **analyzes players' body movements and identifies areas where they could be at risk for injury**. By using generative AI in this way, the Miami Dolphins can identify potential issues before they become serious problems, enabling them to take proactive measures to prevent injuries and improve player performance. 

This technology has the potential to transform the way that sports teams approach player health and performance, improving the overall quality of play and reducing the risk of injury for athletes.

- **Entertainment:** <br>
In the entertainment industry, generative AI can help create new and immersive experiences for users. 

One application of generative AI in entertainment is the **creation of a wide variety of highly realistic and personalized virtual characters and environments for video games and virtual reality experiences**. For example, generative AI can create realistic facial expressions and body movements for virtual characters, enhancing the user experience and immersion. 

Another application of generative AI in entertainment is the **generation of personalized music playlists**. By analyzing a user's listening history and preferences, generative AI can create unique and personalized music playlists that cater to each individual user's tastes. 

This technology can enhance the user experience by creating highly personalized and engaging music experiences. 

**Use case:** <br>
Amper Music uses generative AI to **create original music for video games, films, and other multimedia projects**. Their platform allows users to input parameters such as mood, genre, and tempo, and then generates a unique and original piece of music in real-time. It generates music using a generative model that is trained on a large dataset of musical patterns and structures. By using generative AI to create music, Amper Music can offer highly customizable and original compositions tailored to each individual project's needs. 

This technology has the potential to transform the creation of music for multimedia projects, enabling more efficient and personalized music production.

- **Healthcare** <br>
In healthcare, generative AI can help improve disease diagnosis, medical imaging, and personalized medicine. 

One application of generative AI is in the **generation of synthetic medical images**, which can train machine learning algorithms and improve disease diagnosis. For example, generative AI can generate synthetic medical images of rare diseases, which can be difficult to obtain in real life. 

Another application of generative AI in healthcare is the **creation of medical simulations**. These simulations can **help train healthcare professionals** and reduce medical errors. By **generating synthetic patient data**, generative AI can also **train predictive models and improve personalized medicine**. 

Generative AI has the potential to transform healthcare by enabling the creation of realistic medical simulations, improving disease diagnosis, and enhancing personalized medicine.

**Use case:** <br>
PathAI uses generative AI to improve the accuracy of disease diagnosis through the analysis of medical images. Specifically, PathAI has **developed a deep learning algorithm that can accurately detect cancer cells in digital pathology images**. This algorithm was trained on a large dataset of annotated pathology images using a **variational autoencoder (VAE) generative model**. The VAE learned the underlying structure of the pathology images to generate synthetic images that were like the real images in the training set. By training their algorithm on both real and synthetic images, PathAI was able to improve the accuracy of their cancer detection algorithm. 

This technology has the potential to improve the accuracy and speed of cancer diagnosis, to ultimately improve patient outcomes.

- **Business:** <br>
In business, generative AI can help improve decision-making, personalize customer experiences, and enhance operational efficiency. 

One application of generative AI in business is the **generation of synthetic data**. By generating synthetic data, companies can augment their existing datasets and improve the accuracy of predictive models. For example, a financial institution could generate synthetic financial data to train their predictive models and improve risk management. 

Another application of generative AI in business is the **creation of personalized product recommendations**. By analyzing a customer's purchase history and preferences, generative AI can create personalized product recommendations tailored to each individual customer. This technology can improve customer satisfaction and increase sales. 

People are also using generative AI to improve operational efficiency by creating synthetic data for testing and validation. 
Generative AI has the potential to transform the way businesses operate by improving decision-making, enhancing customer experiences, and increasing efficiency.

 

**Use case:** <br>
Syntiant uses generative AI to **develop low-power, high-performance deep learning chips for a range of industries, including smart home devices, wearables, and voice-controlled devices**. Their deep learning chips are designed to process data on the device itself, rather than relying on cloud-based processing, which can reduce latency and improve privacy. To train their deep learning models, Syntiant uses a **generative adversarial network (GAN) to create synthetic data by generating new data samples** that are similar to the training data. By training their models on both real and synthetic data, Syntiant was able to improve the accuracy of their deep learning chips, making them more reliable and efficient. 

This technology has the potential to transform the way that companies design and manufacture deep learning chips, enabling a new generation of low-power, high-performance devices.

## Limitations of generative AI
As you continue to dive deeper into the world of generative AI, it's crucial to consider the limitations and ethical concerns surrounding these groundbreaking technologies. With models like GPT-4 transforming the way content is created, from text and images to music, it's essential to strike a balance between their extraordinary potential and the need for responsible, equitable usage. 

**Limitations of generative AI**

- **Lack of originality:** Generative AI models rely on large datasets to learn and generate content. As a result, they might not create entirely original content but rather mimic patterns from their training data, which can lead to a lack of creativity and innovation.
- **Incompleteness:** While generative AI models are becoming increasingly sophisticated, they still struggle to understand the nuanced contexts and might generate incomplete or nonsensical content.
- **Bias:** Generative AI models can perpetuate existing biases present in their training data, leading to the generation of biased content that might reinforce stereotypes and discriminatory behavior.
- **Computational resources:** Training and deploying generative AI models require significant computational power, which can be expensive and contribute to environmental concerns such as energy consumption and carbon emissions.

**Ethical concerns of generative AI**

- **Misinformation and fake content:** Generative AI can create convincing fake content, like deepfakes or falsified news articles, which can lead to the spread of misinformation and have severe consequences for individuals and societies.
- **Intellectual property and copyright:** Generative AI can produce content that resembles copyrighted material. This raises questions about intellectual property rights and potential infringements.
- **Privacy:** Generative AI can create realistic images and text about individuals, potentially violating their privacy and causing harm to their reputation.
- **Loss of human touch:** As generative AI becomes more prevalent, there is a risk that the human touch will be lost in various creative domains, potentially leading to a decline in the appreciation of human-created art and culture.
- **Unemployment and job displacement:** The rise of generative AI might lead to job displacement in creative industries, as machines take over tasks previously performed by humans.

Overall, while generative AI has many potential applications and benefits, it’s important to be aware of its limitations and potential pitfalls to use it effectively and ethically.

> Should students in school be permitted to use generative AI tools to complete their class assignments? Why or why not?
Generative AI is a tool that is readily available to everyone. So, consider this as a change in thinking that is occurring.

As students use generative AI, they will need to consider the limitations. For example, students will need to review the generated content to assess if there are any biases, errors, omissions, misinformation, or a lack of originality. And then the student will need to take corrective actions to address those issues.

In addition, instructors know that generative AI is easily available and powerful, so class assignments might change from asking students to write an essay to asking them to assess the essay that generative AI created and report on the embedded limitations, their related implications, and provide solutions to them.

## Future AI trends

You live in the second level of AI, called **Broad AI**, in which machine learning systems have begun to appear in your everyday life. **Broad AI can’t think abstractly, strategize, or use previous experience to come up with new, creative ideas**. But data scientists and programmers are already working on the third level, called General AI. The goal of General AI is to create systems that can perform any intellectual task that a human being can—and more. Some scientists believe that this goal may be reached in about twenty years (the early 2040s).

Scientists at IBM Research predict world-changing advances in AI technology and its relationship with humans. The IBM Research scientists predict that these advances will result in machine-based common sense that can help humans make better judgments.

- **AI EVERYWHERE:** Soon, AI will support seamless connections across industries, ranging from finance, to education, to healthcare. AI will help people work more productively and create new career opportunities.
Industries:
   - Healthcare
   - Finance
   - Agriculture
   - Government
   - Education
   - Energy
   - Science
   - Business solutions
     
- **DEEPER INSIGHTS:** New technologies will be able to sense and analyze things at a level of understanding that was never before possible. This includes future-forward technologies with unfamiliar names, like quantum computing. Quantum computing is a dramatically different way to calculate, based on the behavior of subatomic particles.
Technologies:
   - Quantum computing
   - Distributed deep learning
   - Neuromorphic systems
   - Homomorphic encryption
   - Machine foresight
   - Cognitive discovery

- **ENGAGEMENT REIMAGINED:** New forms of human-machine communication, based on blockchain, conversational bots, and more ideas, will transform how you interact not only with your friends, family members, and coworkers, but also with machines that listen and engage in complex conversations.
Interactions:
   - Human-machine collaboration
   - New AI modalities
   - Augmented reality
   - Global trade logistics
   - Blockchain for payments

- **PERSONILZATION AT SCALE:** Machines will interact with you in ways tailored to your particular desires, habits, and level of comfort. Today, websites already offer merchandise based on orders you’ve made in the past. Soon, you’ll use sites that recognize you and know and understand the things you love. This includes the fashions you follow. Future sites will discuss your fashion preferences with you and offer them perfectly fitted to your size and needs.
- **INSTRUMENTED PLANET:** Billions of sensors generating exabytes of data every day will improve the safety, sustainability, and security of our planet. You won’t get caught in a surprise storm. You will enjoy food from crops grown in new ways that provide maximum taste and nutrition, with minimum damage to the environment.
Connections:
   - Environmental solutions
   - Digital agriculture
   - Connected cars
   - Geospatial temporal data and analytics
   - Smart sensors

